\subsection{Matriz de Erro à Direita}

Considere inicialmente o erro (\ref{eq:er}) e a função de
Lyapunov
%
%\begin{equation}
$2\,V_{r}\!=\!\text{tr}({\bf S}_{r}^{T}\,{\bf S}_{r})$, % = \sum_{i=1}^m{\sum_{j=1}^m{s^2_{r_{ij}}}},$%\end{equation}
%
onde $\text{tr}(\cdot)$ corresponde à função traço. 
%
A partir de (\ref{eq:er}), a derivada temporal de $V_{r}$ ao longo  das
trajetórias do sistema é dada por
%
%\begin{eqnarray}
\begin{equation}
\label{eq:dVr}
2\,{\dot V}_{r} = \text{tr}({\dot {\bf S}}_{r}^{T}\,{\bf S}_{r} + {\bf S}_{r}^{T}\,{\dot {\bf S}}_{r}) %\nonumber \\
                = 2\,\text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta}) + 2\,\text{tr}({{\bf S}}_{r}^{T}\,{\bf K}\,{\dot {\boldsymbol \Theta}}) \,.
\end{equation}
%\end{eqnarray}
%
Em vista de (\ref{eq:dVr}), escolhe-se a seguinte lei de atualização
%
\begin{equation}
\label{eq:dThetaR}
{\dot {\boldsymbol \Theta}} = -{\boldsymbol \Gamma}\,{\bf K}^{T}\,{\bf S}_{r},
\end{equation}
%
onde ${\boldsymbol \Gamma} = {\boldsymbol \Gamma}^{T} > 0 $ é uma matriz de ganho de atualização. Como resultado, tem-se
%
\begin{equation}
\label{eq:dVr1}
{\dot V}_{r} = \text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta})\, \underbrace{- \,\text{tr}({{\bf S}}_{r}^{T}\,{\bf K}\,{\boldsymbol \Gamma}\,{\bf K}^{T}\,{\bf S}_{r})}_{\leq 0}\,,
\end{equation}
%
que depende de $\dot{\bf K}$. Para $\dot{\bf K} \equiv {\bf 0}$, tem-se que $\dot{V}_r \leq 0$. Neste caso, $\dot{\boldsymbol \Theta}={\bf 0}$ e $\dot{V}_r = 0$ quando 
%
\begin{equation}
\label{eq::igualdadesr}
{\bf K}^T\,{\bf S}_r = {\bf 0},
\end{equation}
%
isto é, quando as colunas de ${\bf S}_r$ pertencerem ao espaço nulo à esquerda de ${\bf K}$. Para uma matriz ${\bf K}$ tal que $Nul({\bf K}^T) = \{{\bf 0}\}$, a equação ${\bf K}\,{\boldsymbol \Theta} = {\bf I}$ tem ao menos uma solução ${\boldsymbol \Theta}$ e a igualdade (\ref{eq::igualdadesr}) é verificada apenas para ${\bf S}_r = {\bf 0}$. Porém, para o caso em que o espaço nulo à esquerda possui dimensão igual ou superior a 1, a equação ${\bf K}\,{\boldsymbol \Theta} = {\bf I}$ não tem solução ${\boldsymbol \Theta}$. Note que, neste caso, a igualdade ${\bf K}^T\,{\bf S}_r = {\bf 0}$ corresponde à equação normal para as colunas de ${\boldsymbol \Theta}$, dada por
%
\begin{equation}
{\bf K}^T({\bf K}\,{\boldsymbol \Theta}) = {\bf K}^T({\bf I}),
\end{equation}
% 
e o algoritmo fornecerá a melhor solução pelo critério dos mínimos quadrados.

%\newpage
\subsection{Matriz de Erro à Esquerda}

Similarmente, para o erro (\ref{eq:el}) considera-se a função 
de Lyapunov %, definida positiva
%
%
$2\,V_{\ell}\!=\!\text{tr}({\bf S}_{\ell}^{T}\,{\bf S}_{\ell})$,
%
com derivada temporal ao longo das trajetórias do sistema dada por
%
%\begin{eqnarray}
\begin{equation}
\label{eq:dVl}
2\,{\dot V}_{\ell} = \text{tr}({\dot {\bf S}}_{\ell}^{T}\,{\bf S}_{\ell} + {\bf S}_{\ell}^{T}\,{\dot {\bf S}}_{\ell})% \nonumber \\
                   = 2\,\text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}}) + 2\,\text{tr}({{\bf S}}_{\ell}^{T}\,{\dot {\boldsymbol \Theta}}\,{\bf K}) \,.
\end{equation}
%\end{eqnarray}
%
Novamente, uma escolha para a lei de adaptação é:
%
\begin{equation}
\label{eq:dThetaL}
{\dot {\boldsymbol \Theta}} = -{\boldsymbol \Gamma}\,{\bf S}_{\ell}\,{\bf K}^{T} \,,
\end{equation}
%
onde ${\boldsymbol \Gamma} = {\boldsymbol \Gamma}^T > 0$ é a matriz de ganho de atualização, resultando em
%
\begin{equation}
\label{eq:dVl1}
{\dot V}_{\ell} = \text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}})\,\underbrace{-\, \text{tr}({\bf K}\,{{\bf S}}_{\ell}^{T}\,{\boldsymbol \Gamma}\,{\bf S}_{\ell}\,{\bf K}^{T})}_{\leq 0}\,.
\end{equation}
%
que também depende de $\dot{\bf K}$. Para $\dot{\bf K} \equiv {\bf 0}$, tem-se que $\dot{V}_{\ell} \leq 0$. Neste caso, $\dot{\boldsymbol \Theta}={\bf 0}$ e $\dot{V}_{\ell} = 0$ quando 
%
\begin{equation}
\label{eq::igualdadesl}
{\bf S}_\ell\,{\bf K}^T = {\bf 0},
\end{equation}
%
ou seja, quando as linhas de ${\bf S}_\ell$ pertencerem ao espaço nulo de ${\bf K}$. Para uma matriz ${\bf K}$ tal que $Nul({\bf K}) = \{{\bf 0}\}$, a equação ${\boldsymbol \Theta}\,{\bf K} = {\bf I}$ tem ao menos uma solução ${\boldsymbol \Theta}$ e a igualdade (\ref{eq::igualdadesl}) é verificada apenas para ${\bf S}_\ell = {\bf 0}$. Por outro lado, para o caso em que o espaço nulo possui dimensão igual ou superior a 1, a equação ${\boldsymbol \Theta}\,{\bf K}= {\bf I}$ não tem solução ${\boldsymbol \Theta}$. Note que, neste caso, a igualdade ${\bf S}_\ell\,{\bf K}^T= {\bf 0}$ corresponde à equação normal para as linhas de ${\boldsymbol \Theta}$, dada por 
%
\begin{equation}
({\boldsymbol \Theta}\,{\bf K}){\bf K}^T = ({\bf I}){\bf K}^T.
\end{equation}
%
Novamente, o algoritmo fornecerá a melhor solução pelo critério dos mínimos quadrados.

%\newpage
\subsection{Lei de Atualização Composta}
%Para matrizes $A$ retangulares com posto completo por linhas, {\it{rank($A$)}} $= m$, configura-se um caso de existência, em que o número de soluções para $A x = b$ é $1$ ou $\infty$. Neste caso, a melhor inversa à direita é $C = A^T(A A^T)^{-1}$ e $x = C b$ não apresentará componentes no espaço nulo de $A$. Para matrizes $A$ retangulares com posto completo por colunas, {\it{rank($A$)}} $= n$, configura-se um caso de unicidade. Se houver solução para $A x = b$, ela deve ser $x = BA x = B b$. Pode não haver solução, isto é, o número de soluções é $0$ ou $1$. A melhor inversa à esquerda é $B = (A^TA)^{-1}A^T$ e $p = A(B b)$ corresponde à projeção de $b$ no espaço coluna de $A$. Note que em matrizes retangulares não se pode ter ambas as condições de existência e unicidade. 

Para casos não-redundantes ($m\!=\!n$) e com posto completo, as leis de atualização (\ref{eq:dThetaR}) e (\ref{eq:dThetaL}) são capazes de resolver individualmente o problema de determinação da inversa, uma vez que as matrizes inversa à esquerda e inversa à direita são iguais. Para os casos em que $m\!\neq n$, configuram-se os casos de existência (posto completo por linhas) ou unicidade (posto completo por colunas). Nota-se que as próprias matrizes de erro ${\bf S}_r$ e ${\bf S}_{\ell}$ apresentam dimensões diferentes, sugerindo também propriedades diferentes. A lei de atualização composta que envolve as duas matrizes de erro simultaneamente pode ser obtida a partir da análise de estabilidade da função de Lyapunov $2{V}_c = 2{V}_{r} + 2{V}_{\ell}$, com derivada temporal dada por

\begin{equation}
\label{eq:dVc}
2\,{\dot V}_c = 2\,g(\dot{{\bf K}}) + 2\,\text{tr}({{\bf S}}_{r}^{T}\,{\bf K}\,{\dot {\boldsymbol \Theta}}) + 2\,\text{tr}({{\bf S}}_{\ell}^{T}\,{\dot {\boldsymbol \Theta}}\,{\bf K}) \,,
\end{equation}

\noindent onde $g(\dot{\bf K}) = {\text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}}) + \text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta})}$. Assim, de (\ref{eq:dVc}) tem-se
%
%\begin{equation}
%g(\dot{\bf K}) = {\text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}}) + \text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta})}.
%\end{equation}
%
%Assim, de (\ref{eq:dVc}) tem-se
%
\begin{equation}
{\dot V}_c = g(\dot{\bf K}) + \text{tr}(({{\bf S}}_{r}^{T}\,{\bf K}\, + {\bf K}\,{{\bf S}}_{\ell}^{T}\,){\dot {\boldsymbol \Theta}}).
\end{equation}
%
Desta forma, a lei de atualização proposta é 
%
\begin{equation}
\label{eq:adapcomp}
{\dot {\boldsymbol \Theta}} = -{\boldsymbol \Gamma}( {\bf S}_{\ell}\,{\bf K}^{T} + {\bf K}^{T}\,{\bf S}_{r}) \,.
\end{equation} 
%
onde ${\boldsymbol \Gamma} = {\boldsymbol \Gamma}^T > 0$. Como resultado, tem-se
%
\begin{equation}
{\dot V}_c = g(\dot{\bf K})\,\underbrace{-\,\text{tr}(({{\bf S}}_{r}^{T}\,{\bf K}\, + {\bf K}\,{{\bf S}}_{\ell}^{T}\,){\boldsymbol \Gamma}( {\bf S}_{\ell}\,{\bf K}^{T} + {\bf K}^{T}\,{\bf S}_{r}))}_{\leq 0}.
\end{equation}
%
Para $\dot{\bf K} \equiv {\bf 0}$, tem-se que $\dot{V}_c \leq 0$. Neste caso, $\dot{\boldsymbol \Theta} = {\bf 0}$ e $\dot{V}_c = 0$ quando 
%
\begin{equation}
\label{eq::igualdadecomp}
{{\bf K}^T{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}}\,.
\end{equation} 
%
De imediato, tem-se que
%
%\begin{equation}
%{\bf K}^T\,{\bf S}_r = {\bf 0}\;\wedge\;{\bf S}_\ell\,{\bf K}^T = {\bf 0} \Rightarrow {{\bf K}^T\,{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}}\,,
%\end{equation}
%
a ocorrência simultanea das condições ${\bf K}^T\,{\bf S}_r = {\bf 0}$ e ${\bf S}_\ell\,{\bf K}^T = {\bf 0}$ apresentadas anteriormente implica em $\dot{\boldsymbol \Theta} = {\bf 0}$. %Isto é,
%
%\begin{equation}
%\{{\bf K}^T\,{\bf S}_r = {\bf 0}\} \wedge \{{\bf S}_\ell\,{\bf K}^T = {\bf 0}\} \Rightarrow \{{\bf K}^T{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}\}
%\end{equation}
%
A prova no sentido contrário é obtida a seguir. Assumindo (\ref{eq::igualdadecomp}), tem-se que
%
\begin{eqnarray}
({\bf K}^T\,{\bf S}_r + {\bf S}_\ell\,{\bf K}^T){\bf K}\,{\bf S}^T_\ell &=& {\bf 0}. 
%{\bf K}^T{\bf S}_r {\bf K}{\boldsymbol S}^T_\ell + {\bf S}_\ell\,{\bf K}^T{\bf K}{\boldsymbol S}^T_\ell &=& {\bf 0} \nonumber \\
%{\bf K}^T{\bf K}{\boldsymbol S}_\ell{\boldsymbol S}^T_\ell + {\bf S}_\ell\,{\bf K}^T{\bf K}{\boldsymbol S}^T_\ell &=& {\bf 0}
\end{eqnarray}
%
Utilizando a função traço e uma vez que ${\bf S}_r\,{\bf K} = {\bf K}\,{\bf S}_\ell = {\bf K}\,{\boldsymbol \Theta}\,{\bf K} - {\bf K}$, obtém-se
%
\begin{eqnarray}
0 &=& \text{tr}({\bf K}^T\,{\bf S}_r\,{\bf K}\,{\bf S}^T_\ell + {\bf S}_\ell\,{\bf K}^T\,{\bf K}\,{\bf S}^T_\ell) \nonumber \\
	&=& \text{tr}({\bf K}^T\,{\bf K}\,{\bf S}_\ell\,{\bf S}^T_\ell + {\bf S}_\ell\,{\bf K}^T\,{\bf K}\,{\bf S}^T_\ell) \nonumber \\
	&=& \text{tr}({\bf K}^T\,{\bf K}\,{\bf S}_\ell\,{\bf S}^T_\ell) + \text{tr}({\bf S}_\ell\,{\bf K}^T\,{\bf K}\,{\bf S}^T_\ell) \nonumber \\
	&=& \underbrace{\text{tr}({\bf S}^T_\ell\,{\bf K}^T\,{\bf K}\,{\bf S}_\ell)}_{\geq 0} + \underbrace{\text{tr}({\bf S}_\ell\,{\bf K}^T\,{\bf K}\,{\bf S}^T_\ell)}_{\geq 0}. 
\end{eqnarray}
%
Como todos os termos da soma são positivos ou nulos (traço de matrizes semi-definidas positivas e simétricas), tem-se que $\text{tr}({\bf S}^T_\ell\,{\bf K}^T\,{\bf K}\,{\bf S}_\ell) = \text{tr}({\bf S}_\ell\,{\bf K}^T\,{\bf K}\,{\bf S}^T_\ell) = 0$. Desta forma, tem-se que ${\bf S}_\ell\,{\bf K}^T = {\bf 0}$ e, considerando (\ref{eq::igualdadecomp}), chega-se a ${\bf K}^T\,{\bf S}_r = {\bf 0}$. Assim,
%
\begin{equation}
\{{\bf K}^T\,{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}\} \Leftrightarrow \{{\bf K}^T\,{\bf S}_r = {\bf 0}\} \wedge \{{\bf S}_\ell\,{\bf K}^T = {\bf 0}\}.  
\end{equation}
%
Como visto, a matriz ${\boldsymbol \Theta}$ evoluirá para uma solução que satisfaz (\ref{eq::igualdadecomp}), isto é, 
%
\begin{equation}
\label{eq::sylvester_if}
{\bf K}^T\,{\bf K}\,{\boldsymbol \Theta} + {\boldsymbol \Theta}\,{\bf K}\,{\bf K^T} = 2\,{\bf K}^T.
\end{equation}
%
A equação (\ref{eq::sylvester_if}) é uma equação de Sylvester e apresentará uma solução única se e somente se as matrizes ${\bf K}^T{\bf K}$ e $-{\bf K}\,{\bf K}^T$ não apresentarem autovalores em comum (ver Apêndice \ref{sec::apsylv}). Esta condição é satisfeita para matrizes ${\bf K}$ com o maior posto possível (completo por linhas ou colunas) e a solução converge para a pseudo-inversa de ${\bf K}$. Para os casos em que ${\bf K}$ apresenta posto incompleto, a solução não é única e depende da condição inicial ${\boldsymbol \Theta}(0)$. Na próxima seção, a lei de atualização composta é analisada, sendo justificada a utilização das duas matrizes de erro ${\bf S}_{\ell}$ e ${\bf S}_r$. Além disso, será estabelecida uma relação entre os casos escalar e multivariável.
