\section{\label{sec::convanalysis}Análise de Convergência}

Considere a {\it decomposição de valor singular} ou {\it single value decomposition}(SVD) da matriz de ganho $K(t)\!\in\!\mathbb{R}^{m\times n}$ 
%
\begin{equation}
  K = U\;\Sigma\;V^T,
\end{equation}
%
onde $U\!\in\!\mathbb{R}^{m\times m}$, $\Sigma\!\in\!\mathbb{R}^{m\times n}$ e $V\!\in\!\mathbb{R}^{n\times n}$. As matrizes $U$ e $V$ são matrizes ortogonais cujas colunas são os autovetores de $KK^T$ e $K^TK$, respectivamente, e fornecem bases ortonormais para todos os subespaços fundamentais. A matriz $\Sigma$ possui $r$ valores singulares ($\sigma_i$) em sua diagonal, as raízes dos autovalores não nulos de $KK^T$ e $K^TK$. A SVD escolhe as bases de forma especial. Quando K multiplica uma coluna $v_j$ de $V$, produz $\sigma_j$ vezes uma coluna de $U$. Isto se origina diretamente de 
%
\begin{equation}
K V = U \Sigma,
\end{equation}
%
observado uma coluna por vez \cite{STRANG}. Desta forma, para um sistema (\ref{eq:dy}), torna-se possível relacionar a entrada $u$ e a saída $\dot{y}$. Seja $u$ uma combinação linear das colunas de $V$. A saída $\dot{y}$ será a mesma combinação linear das colunas de $U$, multiplicadas pelos valores singulares $\sigma_i$ a elas associados. Assim, a fatoração SVD será utilizada para análise, se mostrando uma ferramenta muito poderosa. 
%
Da mesma forma, pode-se expressar a matriz $\Theta$ através da sua fatoração SVD:
%
\begin{equation}
  \Theta = U_{\Theta}\;\Sigma_{\Theta}\;V_{\Theta}^T.
\end{equation}
%
A condição $K \Theta \rightarrow I$ pode então ser dividida em:
%
\begin{eqnarray}
\label{c1} U_\Theta  &\rightarrow& V \\
\label{c2} \Sigma \Sigma_\Theta  &\rightarrow& I \\ 
\label{c3} V_\Theta  &\rightarrow& U 
\end{eqnarray}
%
Note que (\ref{c2}) representa um ajuste de amplitudes, similar ao caso escalar e que para a condição $\Theta K \rightarrow I$ se torna $\label{c2} \Sigma_\Theta \Sigma  \rightarrow I $. Porém, adicionalmente a este problema, estão as condições (\ref{c1}) e (\ref{c3}) que estão associadas ao alinhamento das bases ortonormais definidas por $V$ e $U_\Theta$, e também $U$ e $V_\Theta$. Isto é, para o caso multivariável, além do ajuste de amplitudes existente no caso escalar, há também a necessidade do ajuste de ângulos. A solução adotada para análise é expressar o mapeamento definido por $\Theta$ através das bases ortonormais $U$ e $V$. Logo,
%
\begin{equation}
\Theta = V \Psi U^T.
\end{equation}
%
Intuitivamente, o que se faz é utilizar o mapeamento contrário, razoável uma vez que $\Theta$ deve evoluir para a inversa de $K$. Note porém que a matriz $\Psi$ não é diagonal e, assim, para um sinal de entrada igual a uma coluna $u_j$ de $U$, a saída será uma combinação linear das colunas de $V$. 
%A Figura \ref{fig:espacos} ilustra os subespaços fundamentais e o mapeamento realizado pelas matrizes $K$ e $\Theta$.
%
A similaridade entre os casos escalar e multivariável pode ser verificada através da análise dos elementos diagonais e residuais de $\Psi = V^T \Theta U$. Considere o caso mais simples em que $\dot{K}\equiv 0$. A dinâmica de $\Psi$ é dada por
%
\begin{align}
  \dot{\Psi} &= V^T \dot{\Theta} U\,.
\end{align}	
%
Aplicando a lei de atualização composta, proposta em (\ref{eq:adapcomp}), e considerando um ganho escalar $\Gamma > 0$ tem-se
%Using the composite update law proposed in (\ref{eq:adapcomp}) and considering a scalar gain $\Gamma > 0$ we obtain
%
\begin{align}
  \dot{\Psi} &= -\Gamma V^T( S_{\ell} K^{T} + K^{T} S_{r})U \nonumber \\
	&= -\Gamma V^T S_{\ell} K^{T} U - \Gamma V^T K^{T} S_{r} U \notag \\
	&= -\Gamma V^T (\Theta K - I_n)V \Sigma^T - \Gamma \Sigma^T U^T (K \Theta - I_m)U \notag \\
   &= -\Gamma (V^T \Theta U \Sigma - I_n) \Sigma^T - \Gamma \Sigma^T (\Sigma V^T \Theta U - I_m) \notag \\%
	&= -\Gamma (\Psi \Sigma - I_n )\Sigma^T - \Gamma \Sigma^T(\Sigma \Psi - I_m)\,.
\end{align}					
%
A matriz $\Psi$ pode ser decomposta em
%The matrix $\Psi$ can be decomposed as
%
%%% analysis of the dynamics of $\Psi$ is divided in the analysis of its diagonal and off diagonal elements. Thus, it is considered
%
\begin{equation}
  \Psi = D_{\Theta} + R_{\Theta}\,,
\end{equation}
%%
onde $D_{\Theta} \in {\mathbb R}^{n\times m}$ possui os elementos diagonais de $\Psi$ e $R_{\Theta} \in {\mathbb R}^{n\times m}$ possui os elementos fora da diagonal de $\Psi$, denominados residuais, com zeros na diagonal. Por exemplo, para $\Psi \in {\mathbb R}^{3\times 2}$, tem-se
%
\begin{equation}
{\Psi} = \underbrace{\left[\begin{matrix} 
							d_{1}&0\\
							0&d_{2}\\
							0&0\\
							\end{matrix} \right]}_{D_{\Theta}}
						+
							\underbrace{\left[\begin{matrix} 
							0&r_{12}\\
							r_{21}&0\\
							r_{31}&r_{32}\\
							\end{matrix} \right]}_{R_{\Theta}}.
\end{equation}
%
Assim, $\dot{\Psi} = \dot{D_{\Theta}}+\dot{R_{\Theta}}$ e, portanto
%
\begin{align}
  \dot{D}_{\Theta} + \dot{R}_{\Theta} = &- \Gamma ({D_{\Theta}}\Sigma+{R_{\Theta}}\Sigma
    - I_n)\Sigma^T \notag \\%
	&- \Gamma \Sigma^T(\Sigma {D_{\Theta}}+ \Sigma{R_{\Theta}} - I_m)\,.
\end{align}
%
Note que o produto de matrizes diagonais e matrizes residuais também são residuais, apresentando zeros na diagonal principal. Assim, é possível separar a análise da dinâmica de $D_{\Theta}$ e $R_{\Theta}$. Para a diagonal de $\Psi$, tem-se
%
\begin{equation}
  \dot{D}_{\Theta} = -\Gamma (D_{\Theta} \Sigma - I )\Sigma^T
    - \Gamma \Sigma^T(\Sigma D_{\Theta} - I )\,.
\end{equation}
%
A dinâmica do elemento diagonal $d_i$ é dada por
%
\begin{align}
  \dot{d}_i &= - \Gamma (d_i \sigma_i - 1)\sigma_i - \Gamma \sigma_i(\sigma_i d_i - 1) \notag \\
		    &= - 2\Gamma S_i\sigma_i\,,
\end{align}
%
apresentando uma forma similar ao caso escalar analisado inicialmente ( note que $\sigma_i \in \mathbb{R}^{+}$). Note também que
%
\begin{align}
  \dot{d}_i &= - 2\Gamma\sigma_i^2\,d_i + 2\Gamma \sigma_i\,,
\end{align}
%
isto é, os elementos da diagonal convergem exponencialmente para $1/\sigma_i$. A Figura \ref{figuranova} apresenta a evolução de $d_i$ para diferentes condições iniciais.

\begin{figure}[htpb]
\centering
\def\Ga{\framebox{\parbox[c]{12mm}{$S_i>0$ \\ $\dot{d_i}<0$}}}
\def\Gb{\framebox{\parbox[c]{12mm}{$S_i<0$ \\ $\dot{d_i}>0$}}}
\def\JPicScale{0.65}
{\small
\input{fig/plane3.pst}
}
\caption{Plano $d_i \times \sigma_i$ (diferentes condições iniciais).}
\label{fig:locusS3}
\end{figure}

Para os elementos residuais tem-se 
%
\begin{equation}
  \dot{R_{\Theta}} = -\underbrace{{\Gamma R_{\Theta} \Sigma\Sigma^T }}_{1}
    - \underbrace{{\Gamma \Sigma^T\Sigma R_{\Theta}}}_{2}\,.
\end{equation}
%
As matrizes $\Sigma\Sigma^T$ e $\Sigma^T\Sigma$ são matrizes diagonais e quadradas cujios elementos são os quadrados dos valores singulares de $K$. Para os casos em que $m \neq n$, apresentam dimensões diferentes e uma delas também apresentará zeros na diagonal. Por exemplo, para $\Sigma \in {\mathbb R}^{2\times 3}$:
%
\begin{equation}
{\Sigma\Sigma^T} = \left[\begin{matrix} 
							\sigma_{1}^2&0\\
							0&\sigma_{2}^2\\
							\end{matrix} \right]
							\quad\quad \text{e} \quad\quad
{\Sigma^T\Sigma} = \left[\begin{matrix} 
							\sigma_{1}^2&0&0\\
							0&\sigma_{2}^2&0\\
							0&0&0\\
							\end{matrix} \right].			
\end{equation}
%
No termo 1, obtido quando considerada a matriz de erro $S_\ell$, os elementos diagonais multiplicam as colunas de $R_\Theta$. Por outro lado, no termo 2, os elementos diagonais são aplicados as linhas de $R_\Theta$. Assim, a dinâmica de um elemento residual $r_{ij}$ pode ser expressa por
%
\begin{equation}
  \dot{r}_{ij} + \Gamma(\sigma_i^{*2}+\sigma_j^{*2})r_{ij} = 0\,,
\end{equation}
%
onde $\sigma_k^* = \sigma_k$ se $k \leq \min(m,n)$ e $\sigma_k^* = 0$ caso contrário. A utilização de ambas as matrizes de erro $S_\ell$ e $S_r$ torna a convergência do elemento residual dependente de dois valores singulares distintos, associados à linha e à coluna. Por exemplo, considere o caso onde $m=n$ e $\sigma_m = 0$. O uso de uma lei de atualização baseada apenas em $S_r$ (multiplicação das linhas) resultaria em $\dot{r}_{mj} = 0,\;\forall j$. Os elementos residuais de toda $m$-ésima linha não podem se alterar e portanto não se tornarão nulos. O algoritmo composto que também utiliza $S_\ell$ (multiplicação das colunas) assegura a convergência desses elementos residuais para $0$ desde que $\sigma_j \neq 0$. Para os casos onde $m\neq n$ a necessidade de uma lei de atualização composta se torna ainda mais evidente. Considere o caso $K \in {\mathbb R}^{2\times 3}$ e com posto completo por linhas
%
\begin{equation}
{K} = \left[\begin{matrix} %
							\vdots&\vdots\\
							u_1&u_2\\%
							\vdots&\vdots\\
							\end{matrix} \right]
\left[\begin{matrix} 
							\sigma_1&0&0\\
							0&\sigma_2&0\\
							\end{matrix} \right]
\left[\begin{matrix} 
							\vdots&\vdots&\vdots\\
							v_1&v_2&v_3\\%
							\vdots&\vdots&\vdots\\
							\end{matrix} \right]^T,
\end{equation}
%
onde, portanto, $\sigma_1 \neq 0$ e $\sigma_2 \neq 0$. A última coluna de $V$, $v_3$, fornece uma base ortonormal para o espaço nulo de $K$, isto é, seja $b = \alpha v_3$, tem-se $Kb = 0\;\forall\;\alpha$. Considere agora a expressão da matriz $\Theta$ como
%
\begin{equation}
{\Theta} = \left[\begin{matrix} %
							\vdots&\vdots&\vdots\\
							v_1&v_2&v_3\\%
							\vdots&\vdots&\vdots\\
							\end{matrix} \right]
\left[\begin{matrix} 
							d_1&r_{12}\\
							r_{21}&d_2\\
							r_{31}&r_{32}\\
							\end{matrix} \right]
\left[\begin{matrix} 
							\vdots&\vdots\\
							u_1&u_2\\%
							\vdots&\vdots\\
							\end{matrix} \right]^T.
\end{equation}
%
Note que $r_{31}$ e $r_{32}$ estão associados a esta base (multiplicam $v_3$) e, para $r_{31} \neq 0$ e $r_{32} \neq 0$, uma lei $b = \Theta c$ apresentará componentes no espaço nulo de $K$, não representando portanto uma lei de controle ótima. Como foi visto, apenas a utilização de $S_\ell$ permitirá que estes elementos evoluam para zero, uma vez que torna a convergência dependente do valor singular associado à coluna. Considere agora o caso $K \in {\mathbb R}^{3\times 2}$ e com posto completo por colunas ($\sigma_1 \neq 0$ e $\sigma_2 \neq 0$). Da mesma forma, $K$ pode ser escrito como
%
\begin{equation}
{K} = \left[\begin{matrix} %
							\vdots&\vdots&\vdots\\
							u_1&u_2&u_3\\%
							\vdots&\vdots&\vdots\\
							\end{matrix} \right]
\left[\begin{matrix} 
							\sigma_1&0\\
							0&\sigma_2\\
							0&0\\
							\end{matrix} \right]
\left[\begin{matrix} 
							\vdots&\vdots\\
							v_1&v_2\\%
							\vdots&\vdots\\
							\end{matrix} \right]^T.
\end{equation}
%
Neste caso, a última coluna de $U$, $u_3$, fornece uma base ortonormal para o espaço nulo à esquerda de $K$. Isto é, para um sistema $z = K b$, não existe solução $b$ para $z = \alpha u_3$. Representa portanto uma base para variáveis $z$ que são inalcançáveis ou inviáveis. Considere a expressão de $\Theta$ como
%
\begin{equation}
{\Theta} = \left[\begin{matrix} %
							\vdots&\vdots\\
							v_1&v_2\\%
							\vdots&\vdots\\
							\end{matrix} \right]
\left[\begin{matrix} 
							d_1&r_{12}&r_{13}\\
							r_{21}&d_2&r_{23}\\
							\end{matrix} \right]
\left[\begin{matrix} 
							\vdots&\vdots&\vdots\\
							u_1&u_2&u_3\\%
							\vdots&\vdots&\vdots\\
							\end{matrix} \right]^T.
\end{equation}
%
Os elementos residuais $r_{13}$ e $r_{23}$ estão associados a $u_3$. Para $r_{13}\neq 0$, $r_{23}\neq 0$ e $c = \alpha u_3$, tem-se $b = \Theta c \neq 0$, embora $c$ seja uma saída inviável para o sistema. Apenas a utilização da matriz de erro à direita $S_r$ garantirá que estes elementos residuais convirjam para $0$ e assim $b = \Theta c = 0$. 
  
%where $\sigma_k^* = \sigma_k$ if $k \leq \min(m,n)$ and $\sigma_k^* = 0$ otherwise (rectangular matrices). The use of the two error matrices $S_\ell$ and $S_r$ makes the residue convergence dependent of two distinct single values, related to the row and column. For instance, consider the case where $m=n$ and $\sigma_m = 0$. The use of an update law based only on $S_r$ (with zeros multiplying the lines) would result in $\dot{r}_{mj} = 0,\;\forall j$. The residual elements of the entire $m^{th}$ line cannot change and therefore cannot become null. The composite algorithm which also uses $S_{\ell}$ (with zeros multiplying the columns) ensures the convergence of these residues to 0 since $\sigma_j \neq 0$. For the $m<n$ case, with full rank, the residual elements $r_{ij}$ for $i>m$ are related with vectors in the nullspace of $K$. The use of $S_\ell$ ensures that these elements converge to zero. For the case where $m>n$, the residuals $r_{ij}$ for $j > n$ are associated with vectors in nullspace of $K^T$ and only the use of $S_r$ ensures convergence of these residual terms to 0.


%%% \section{Modification (OLD)}
%%%
%%% Nas simulações, além da lei controle inicialmente proposta em \eqref{eq:um}, dada por $u_{_I}\!=\!\Theta\,v_r$, considera-se também a seguinte lei de controle modificada:
%%% %
%%% \begin{equation}
%%% \label{eq:umodifi}
%%% u_{_{II}}=\Theta\,\Theta^TJ^T\,v_r\,.
%%% \end{equation}
%%% %
%%% Esta modificação é motivada a partir da análise da função de Lyapunov $2V_e = e^T\Lambda e$, cuja derivada com respeito ao tempo, ao longo das trajetórias do sistema, é dada por $\dot{V}_e = e^T\Lambda(\dot{r} - J\,u)$. Então, para $u = u_{_{II}}$, obtém-se a seguinte equação:
%%% %
%%% \begin{equation}
%%% \label{eq:dVe}
%%% \dot{V}_e = e^T\Lambda(I - P\,P^T)\,\dot{r}\,\underbrace{-\,e^T\Lambda\,P\,P^T\Lambda\,e}_{\Phi}\,,
%%% \end{equation}
%%% %
%%% onde $P = J\,\Theta$. Note que o termo $\Phi$ corresponde a uma parcela semi-definida negativa, independente da qualidade da estimativa da inversa dada pelo algoritmo de atualização \eqref{eq:adapcomp}. Além disso, a norma da matriz $\Lambda$ pode ser ajustada para que o termo $\Phi$ prevaleça sobre o termo à esquerda de \eqref{eq:dVe}.













