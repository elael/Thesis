
\section{Probabilistic Regression}

Probabilistic regression is similar to classification, both infer properties of
a sample based on previous information. However, instead of giving a definite
answer for which class an element belongs, probabilistic regression gives the
probability for such a classification~\cite{jaakkola1999probabilistic}. More
formally, a training set is a sequence of $n$ pairs
$\{(x_i,y_i)|~i=1,\ldots,n\}$, where $x_i$ and $y_i$ are samples drawn from
random variables $X$ and $Y$, respectively, with joint probability distribution
$\Pr(X,Y)$~\cite{friedman2001elements}. From this training set, a conditioning
probability $\Pr(Y|X=x)$ has to be estimated.

The special case where $Y$ is a Bernoulli random variable, i.e. a binary
variable, is called binary regression. It is the single most important
regression for mapping, as such, no other kind is explored in this thesis. 

\subsection{Binary Logistic Regression} 
\label{ss:blr}
A binary regression dependent variable $Y\in\{-1,1\}$
(or some set of equal cardinality like $\{0,1\}$) has two
possible estimations that are related by $\Pr(Y=1|X=x)+\Pr(Y=-1|X=x)=1$. As
such, the conditional probability can be denoted
\begin{equation*}
p(x) = \Pr(Y=1|X=x) 
\end{equation*}

The probability for $Y=-1$ can be recovered from $P(Y=-1|X=x)=1-p(x)$. The
linear logistic model~\cite{ramos2016hilbert} $p(x;\coord{w})$ for
$x\in\mathbb{R}^d$, with $\coord{w}\in\mathbb{R}^d$ as explicit parameter:

\begin{equation}
\label{eq:std_linear_logistic}
p(x;\coord{w}) = \frac{1}{1+\exp(-\coord{w}\cdot x)}
\end{equation}

The rationale behind the model is that the function
$(1+\exp(-\alpha))^{-1}$ is a bijection
$\mathbb{R}\to(0,1)$~\cite{friedman2001elements}. Ensuring that the model is a
probabilistic distribution.

The classical regression theory requires a loss function $\mathscr{C}$ and
minimizes an empirical risk over some space of functions
$R[f]=\mathbb{E}(\mathscr{C}(X,Y,f(X)))$~\cite{jaakkola1999probabilistic},
where $\mathbb{E}(\parm)$ is the expected value.
The estimation of $f$ from samples $(x_i,y_i)$ uses a regularized version:
\begin{equation}
\label{eq:regularized_estimator}
R_{\text{reg}}[f] =
\frac{1}{n}\sum_{i=1}^n\mathscr{C}(x_i,y_i,f(x_i))+\lambda\mathcal{S}[f]\,,
\end{equation}
%
where $\lambda>0$ and $\mathcal{S}[\parm]$ stabilization (regularization) term,
as the minimization problem is typically
ill-posed~\cite{jaakkola1999probabilistic}. On normed spaces usually
$\mathcal{S}[f]=g(\norm{f})$, where $g(\parm)$ is a monotonically increasing
function. With proper adjustment of $\lambda$, the $\nicefrac{1}{n}$ factor
might be ignored without changing the minimizing function.
The loss function $\mathscr{C}$ is problem dependent and for binary regression a
typical negative log likehood (NLL\abbrev{NLL}{Negative Log Likehood}) is used
\begin{equation}
\label{eq:general_nll}
\mathscr{C}(x,y,p(x)) = - \log \Pr(Y=y|X=x) = 
\begin{cases} 
      - \log p(x) & y=1 \\
      - \log (1-p(x)) & y=-1
   \end{cases}
\end{equation}

For a linear logistic model, one uses (\ref{eq:std_linear_logistic}) on
(\ref{eq:general_nll}) and substitutes back into
(\ref{eq:regularized_estimator}).
Thus the regularized negative log likehood empirical risk simplifies to a $d$
dimensional minimization:
\begin{equation}
\label{eq:classicnll}
\text{NLL}_{\text{reg}}(\coord{w}) =  \sum_{i=1}^n \log(1+\exp(-y_i
\coord{w}\cdot x_i)) +\lambda\mathcal{S}(\coord{w})\,.
\end{equation}


Reasonable choices for $\mathcal{S}(\parm)$
are the $\ell_1$ norm (LASSO)
$\mathcal{S}(\coord{w})=\norm{\coord{w}}_1$
or elastic net, a combination of $\ell_1$ (LASSO) and
squared $\ell_2$ norms (ridge),
$\mathcal{S}(\coord{w})=\alpha_1\norm{\coord{w}}_1
+\alpha_2\norm{\coord{w}}_2^2$, where
$\alpha_1+\alpha_2=1$, $\alpha_1,\alpha_2\in[0,1]$~\cite{hastie2015statistical}.


Although it is an applicable setting for simple situations, it is not expected
to perform well for classification/regression of point on a 3D environment as
$x\in\mathbb{R}^3$ and $\coord{w}\in\mathbb{R}^3$. The three real parameters,
encoded by $\coord{w}$, is not enough to capture all the complexities of the
environment. To keep the simplicity provided by the linear model and still be
suitable for a three dimensional map, an alternative is to increase
dimensionality using Hilbert Spaces.

\subsection{Regression on Hilbert Spaces}
\label{ss:reg_hs}
Samples defined on a low dimensional space $\mathcal{X}$, e.g. $\mathbb{R}^3$,
can be raised to a high (possible infinite) dimensional space using a feature
map $\varphi:\mathcal{X}\to\hs$, where $\hs$ is hilbert space. This allow
linear models to express more generic functions $f(x) =
\hip{\coord{w}}{\varphi(x)}$ with $\coord{w}\in\hs$, such a space of functions
$f(x)$ is the actual RKHS with kernel define as in equation
(\ref{eq:feature_kernel}).

The linear logistic model from equation (\ref{eq:std_linear_logistic}) lifted to
the Hilbert Space $\hs$ is
\begin{equation}
\label{eq:hslogistic}
p(x;\coord{w}) = \frac{1}{1+\exp(-\hip{\coord{w}}{\varphi(x)})}
\end{equation}

Thus, equation (\ref{eq:classicnll}) for log likehood empirical risk
becomes\footnote{With appropriate adjustment of $\lambda$'s value,i.e.
$\lambda=n\lambda_0$, the usual average $\nicefrac{1}{n}$ for the summation can
be dropped, as it keeps the same minimizer value of $\coord{w}$.}
\begin{equation}
\label{eq:hsnll}
\text{NLL}_{\text{reg}}(\coord{w}) =  \sum_{i=1}^n \log(1+\exp(-y_i
\hip{\coord{w}}{\varphi(x_i)})) +\lambda\mathcal{S}(\coord{w})
\end{equation}

In practice, one chooses a kernel $K(x,y)$ with desired properties and
find finite dimensional approximate features $\hat\varphi(x)$, such that
$K(x,y)=\hip{\varphi(x)}{\varphi(y)}\approx\hat\varphi(x)\cdot\hat\varphi(y)$.
Non kernel specific methods for finding approximate features include sampling
the Fourier transform of shift invariant kernels~\cite{rahimi2007random}, i.e.
$K(x,y)=k(x-y)=k(\delta)$, and Nystr\"om method that projects the Gram matrix
$\mathbf{G}_{ij}=K(x_i,x_j)$ of the sample points $\{x_i\}$ into some subset of
these points and use this projection to approximate the feature
maps~\cite{williams2000using}.
