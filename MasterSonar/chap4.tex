\chapter{Inversa Filtrada}
%\begin{figure}[!htp]
%\centering
%\includegraphics[trim = 2.7cm 0cm 2.7cm 0cm, scale = 0.49]{sim/sim1_qdot.eps}
%\caption{aquii}
%\end{figure}

No capítulo \ref{chap::sing} foram apresentados dois algoritmos alternativos e consolidados 
para solucionar o problema de cinemática inversa de manipuladores robóticos na presença de configurações singulares. 
O método DLS propõe a utilização de um termo de amortecimento, estabelecendo um compromisso entre a exatidão e a viabilidade da solução. O algoritmo FIK, por sua vez, emprega um laço de realimentação para minimizar a diferença entre as velocidades atual e desejada no espaço operacional, não exigindo inversões. Neste capítulo, é introduzido o algoritmo proposto para solucionar tal problema. A idéia principal por trás do método consiste em calcular a inversa dinamicamente. A análise tem início, portanto, com o caso mais simples, escalar, sendo natural sua extensão para o caso multivariável.

\section{Algoritmo Proposto}

Considere uma planta SISO não-linear de $1^{a}$ ordem  descrita por
%
\begin{equation}
\label{eq:dy}
{\dot y} = k(t)\,u \,,
\end{equation}
%
onde $u\!\in\!{\mathcal L}_{\infty}$ é a entrada escalar da     planta
ou a variável de controle do sistema, $y$ é a saída da planta e $k(t)$
é uma função escalar não-linear.
%
Assumindo que o objetivo de controle seja rastrear uma trajetória   de
referência desejada $y_d(t)$, uma lei de controle $u$ que      lineariza
o sistema e garante a estabilidade assintótica do erro de rastreamento
$e\!:=\!y_d-y$ é dada por
%
\begin{equation}
\label{eq:u}
u = k^{-1}(t)\,\nu\,, \qquad \nu ={\dot y}_d + \lambda\,e\,,
\end{equation}
% \,, \qquad {\boldsymbol \nu}={\dot{\bf y}}_d + {\boldsymbol \Lambda}\,{\bf e} \,,
onde $\lambda > 0$. Como foi visto, o erro de rastreamento tenderá exponencialmente a $0$ de acordo com o valor do ganho $\lambda$ utilizado e a presença do termo {\it feedforward} $\dot{y}_d$ garante a manutenção do erro em $0$ independente do tipo de sinal $y_d$ (constante ou variante no tempo). 
%
Agora, considere uma lei de controle $u$ que não utilize a inversa, computada instantaneamente como $1/k(t)$, mas uma função $\theta(t)$ atualizada dinamicamente tal que
%Agora, considere que $\theta(t)$ não seja computada instantaneamente como $1/k(t)$, mas seja atualizada dinamicamente tal que 
%
\begin{equation}
\label{eq::idea}
k\theta \to 1.
\end{equation}
%
Para estabelecer uma dinâmica adequada para $\theta(t)$, satisfazendo (\ref{eq::idea}),
introduz-se o seguinte sinal de erro não-linear
%
\begin{equation}
\label{eq:S}
S = k \theta - 1 \,.
\end{equation}
%
Considera-se a função de Lyapunov $2V(S)\!=\!S^{2}$ cuja derivada temporal é dada por
%
\begin{equation}
\label{eq:dV}
{\dot V}(S) = S\,{\dot S} = S\,({\dot k}\,\theta + k\,\dot{\theta}) \,.
\end{equation}
%
Em vista de (\ref{eq:dV}), escolhe-se a seguinte lei de atualização paramétrica
%
\begin{equation}
\label{eq:dtheta}
{\dot \theta} = -\beta\,S\,k \,,
\end{equation}
%
onde $\beta\!>\!0$, e como resultado obtém-se
%
\begin{equation}
\label{eq:dV1}
{\dot V}(S) = S\,{\dot k}\,\theta - \beta\,S^{2}\,k^{2} \,.
\end{equation}
%
%---------------------------------------------------------------------
Note que, devido ao termo $S\dot{k}\theta$, $\dot{V}(S)$ não tem sinal definido.  
%A seguir serão analisadas algumas possibilidades. 
Por agora, suponha o caso mais simples em que ${\dot k}\equiv0$. Neste caso, $\dot{V}(S) \leq 0$ e $k \equiv 0$ também representará um ponto de equilíbrio. % e $\dot{V}(S) = 0$ para $S = 0$ e/ou $k = 0$.
A lei de atualização (\ref{eq:dtheta}) pode ser reescrita como
%
\begin{equation}
\label{eq:dtheta1}
{\dot \theta} = -\beta\,(\,k\theta - 1 )\,k = -\beta\,k^{2}\,\theta + \beta\,k \,,
\end{equation}
%
e, utilizando o operador diferencial $s$, 
%
\begin{equation}
\label{eq:dtheta1b}
(s + \beta\,k^{2})\theta = \beta\,k \,.
\end{equation}
%
Para $k \neq 0$, pode-se explicitar $\theta$ como
%
\begin{equation}
\label{eq:theta}
\theta = \frac{\beta k}{s + \beta\,k^{2}} = \frac{1}{\tau s + 1}\,\frac{1}{k}\,,%[{1}/{k}]\,,
\end{equation}
%
onde 
%
%\begin{equation}
$\tau\!=\!1/\beta k^{2}.$
%\end{equation}
%
Deste modo, $\theta$ pode ser interpretado como a saída de um filtro
linear onde a entrada é a inversa $1/k$.
%
Por isso, o sinal $\theta$ será denominado de \emph{inversa filtrada} da função $k$.
%
Quanto menor é o valor de $k$, maior é a constante 
$\tau$ e, consequentemente, mais  lento é o filtro. O mesmo se verifica para o ganho $\beta$% utilizado na lei de atualização
: quanto menor este ganho, maior será a constante de tempo do filtro.

\section{Propriedades da Inversa Filtrada}
%
Uma propriedade importante do algoritmo proposto é que para $k\equiv0$
tem-se que ${\dot \theta}\!\equiv\!0$ e, como            consequência,
$\theta(t)\!=\!\theta(0)$.
%
A Figura \ref{fig:locusS1} ilustra o plano $\theta\times k$ com o lugar
geométrico  de $S\!=\!0$ e algumas trajetórias para         diferentes
condições  iniciais $\theta(0)$. De acordo com a figura        pode-se
afirmar que para qualquer $k$ constante a saída $\theta(t)$ é limitada.

\begin{figure}[htpb]
\centering
\def\Ga{\framebox{\parbox[c]{12mm}{$S>0$ \\ $k>0$ \\ $\dot{\theta}<0$}}}
\def\Gb{\framebox{\parbox[c]{12mm}{$S<0$ \\ $k>0$ \\ $\dot{\theta}>0$}}}
\def\Gc{\framebox{\parbox[c]{12mm}{$S>0$ \\ $k<0$ \\ $\dot{\theta}>0$}}}
\def\Gd{\framebox{\parbox[c]{12mm}{$S<0$ \\ $k<0$ \\ $\dot{\theta}<0$}}}
\def\JPicScale{0.65}
{\small
\input{fig/plane1.pst}
}
\caption{Plano $\theta \times k$ (diferentes condições iniciais).}
\label{fig:locusS1}
\end{figure}

Considere agora o caso de uma função $k(t)$ variante no tempo.       A
partir da figura é evidente que a única possibilidade para $\theta(t)$
crescer sem limites está na região delimitada por $-1\!<S\!<0$, isto é,
na região sombreada no primeiro quadrante (Figura \ref{fig:locusS2}).

\begin{figure}[htpb]
\centering
\def\Ga{\framebox{\parbox[c]{12mm}{$S>0$ \\ $k>0$ \\ $\dot{\theta}<0$}}}
\def\Gb{\framebox{\parbox[c]{12mm}{$S<0$ \\ $k>0$ \\ $\dot{\theta}>0$}}}
\def\Gc{\framebox{\parbox[c]{12mm}{$S>0$ \\ $k<0$ \\ $\dot{\theta}>0$}}}
\def\Gd{\framebox{\parbox[c]{12mm}{$S<0$ \\ $k<0$ \\ $\dot{\theta}<0$}}}
\def\JPicScale{0.65}
{\small
\input{fig/plane2b.pst}
}
\caption{Plano $\theta \times k$ (escape).}
\label{fig:locusS2}
\end{figure}

%
Note que, quanto mais próximo de zero é o valor de $k$, maior pode ser
o valor de $\theta$. Entretanto, pode-se verificar que nesta    região
$0\!<\!|k|\!<\!1$ e $0\!<\!|{\dot \theta}|\!<\!\beta$, implicando  que
$\theta$ não apresenta escape em tempo finito.
%
%%% \begin{figure}[htpb]
%%% \centering
%%% \psfrag{t}[c][c][0.8]{$\theta$}
%%% \psfrag{k}[c][c][0.8]{$k$}
%%% \psfrag{S1}[c][c][0.8]{$S\!=\!0$}
%%% \psfrag{S2}[c][c][0.8]{$S\!=\!-1$}
%%% \psfrag{S3}[c][c][0.8]{$S\!<\!0$}
%%% \psfrag{SA}[c][c][0.8]{$\begin{array}{c}S>0\\k>0\\{\dot \theta}<0\end{array}$}
%%% \psfrag{SB}[c][c][0.8]{$\begin{array}{c}S<0\\k<0\\{\dot \theta}<0\end{array}$}
%%% \psfrag{SC}[c][c][0.8]{$\begin{array}{c}S>0\\k<0\\{\dot \theta}>0\end{array}$}
%%% \psfrag{SD}[c][c][0.8]{$\begin{array}{c}S<0\\k>0\\{\dot \theta}>0\end{array}$}
%%% \includegraphics[width=6cm]{./figeps/planetk.eps}
%%% \caption{Plano $\theta \times k$.}
%%% \label{fig:locusS}
%%% \end{figure}
%
% Um exemplo, onde $k\to0$ e $\theta\to\infty$, pode ser formulado   a
% seguir.
%
Considera-se agora o caso em que $k$ tende para zero. A seguir será construído um exemplo mostrando que, neste caso, $\theta$ pode crescer sem limites mas de maneira muito lenta. Considere uma trajetória no interior da região        sombreada
satisfazendo $V\!=\!S^{2}\!=\!f(t)$, onde $f(t)$ é uma função contínua
tal que $0\!<\!f(t)\!<\!1$ para $\forall t$. Como $-1\!<\!S\!<\!0$   e
$k\!>\!0$, segue de (\ref{eq:S}) que
%
\begin{equation}
\label{eq:thetaf}
\theta = \frac{1 - \sqrt{f}}{k} \,,
\end{equation}
%
Por sua vez, como ${\dot V}\!=\!{\dot f}$, a partir de (\ref{eq:dV}) e sabendo que $S\!\neq\!0$ tem-se:
%
\begin{equation}
\label{eq:dk}
{\dot k} = \frac{{\dot f} + \beta\,f\,k^{2}}{S\,\theta} \,.
\end{equation}
%
Então, substituindo (\ref{eq:thetaf}) em (\ref{eq:dk}) obtém-se:
%
\begin{equation}
\label{eq:dk1}
{\dot k} = \frac{{\dot f} + \beta\,f\,k^{2}}{f-\sqrt{f}}\,k \,.
\end{equation}
%
Para uma função constante $f(t)\!=\!c$,  com $0\!<\!c\!<\!1$,       a
equação (\ref{eq:dk1}) assume a forma:
%
\begin{equation}
\label{eq:dk2}
{\dot k} = -a\,k^{3} \,, \qquad a = \frac{\beta \sqrt{c}}{1-\sqrt{c}} \,.
\end{equation}
%
Portanto, a solução de (\ref{eq:dk2}) é dada por:
%
\begin{equation}
\label{eq:k}
k(t) = k(0)\,\sqrt{\frac{1}{2at\,k^{2}(0) + 1}} \,,
\end{equation}
%
que converge para zero mais lentamente que qualquer exponencial.
%
As propriedades do algoritmo (\ref{eq:dtheta}) podem ser resumidas como:
%
\begin{description}
\item[(P1)] $k\equiv0 \,\,\, \Rightarrow \,\,\, \theta(t) = \theta(0)$.
\item[(P2)] $k$ constante \,\,\, $\Rightarrow \,\,\, \theta \in \mathcal{L}_\infty$.
\item[(P3)] $k \in \mathcal{L}_{\infty} \,\,\, \Rightarrow \,\,\, {\dot \theta} \in \mathcal{L}_\infty$.
\item[(P4)] $\theta(t)$ não possui escape em tempo finito.
\end{description}
%
Como visto, a idéia do algoritmo é simples: calcular a inversa dinamicamente. A utilização de apenas um ganho para atualização de $\theta$ surge como uma facilidade de sintonia do método e a condição verificada para a aplicação do método é que $k$ não tenda para $0$ mais lentamente do que uma exponencial.

\begin{simulation}
\label{sim::sisosimple}
\end{simulation}
\noindent Considere os ganhos $k_1 = 2$, $k_2= 0.5$, $k_3=0$ e $k_4(t) = 2\,e^{-0.5\,t}$. Na Figura \ref{fig::simsiso_simple}, são apresentados os sinais $\theta_i$ obtidos para $\beta = 5$ e $\theta_i(0) = 1$.
%Como visto nas Figuras \ref{fig::siso}.a e \ref{fig::siso}.b, a medida que o ganho $\beta$ aumenta, menor é a constante de tempo do filtro e a variável $\theta$ atinge valores mais elevados para $k(t) = 0$ . A Figura \ref{fig::siso}.c mostra o plano $k \times \theta$ para os diferentes ganhos utilizados. Uma vez que $\dot{k} < \infty$, é possível verificar que $\partial \theta/\partial k = 0$ para $S = 0$ e $k = 0$.

\begin{figure}[!htb]
\includegraphics[trim = 2.7cm 9.15cm 0cm 1cm, clip = true, scale = 0.49]{sim/simsiso_simple_exp.eps}
\caption{Simulação \ref{sim::sisosimple}: evolução de (a) $\theta$ e (b) $S$ para diferentes ganhos $k$.}
\label{fig::simsiso_simple}
\end{figure}
%
%Como visto, a rapidez do filtro está relacionada ao ganho $k$ e, para $k$ nulo, tem-se que $\theta(t) = \theta(0)$.

%\newpage
\begin{simulation}
\label{sim::siso}
\end{simulation}
\noindent Considere agora um ganho variante no tempo $k(t) = sin(0.4\pi t)$, $\theta(0) = 0$ e $\beta = \{10, 100, 1000\}$. 
Como visto nas Figuras \ref{fig::siso}.a e \ref{fig::siso}.b, a medida que o ganho $\beta$ aumenta, menor é a constante de tempo do filtro e a variável $\theta$ atinge valores mais elevados para $k$ próximo ou igual a zero. A Figura \ref{fig::siso}.c mostra o plano $\theta \times k$ para os diferentes ganhos utilizados e, uma vez que $\dot{k} < \infty$, é possível verificar que $\partial \theta/\partial k = 0$ para $S = 0$ e $k = 0$.

\begin{figure}[!htb]
\includegraphics[trim = 2.7cm 0cm 0cm 0.6cm, scale = 0.49]{sim/simsiso_data.eps}
\caption{Simulação \ref{sim::siso}: variáveis (a) k e $\theta$; (b) S; e (c) plano $\theta \times k$.}
\label{fig::siso}
\end{figure}

Os resultados obtidos para o rastreamento de $y_d = 4 + cos(0.2\pi t)$, com ganho ganho $\lambda = 10$ e $y(0) = 5$, são apresentados na Figura \ref{fig::siso_data2}. Nota-se que quanto maior o ganho de atualização utilizado, melhor será o desempenho. 

\begin{figure}[!htb]
\includegraphics[trim = 2.7cm 0cm 0cm 0cm, scale = 0.49]{sim/simsiso_y.eps}
\vskip 0.15cm
\includegraphics[trim = 2.7cm 0cm 0cm 0cm, scale = 0.49]{sim/simsiso_u.eps}
\caption{Simulação \ref{sim::siso}: variáveis (a) y e (b) u para diferentes ganhos $\beta$.}
\label{fig::siso_data2}
\end{figure}

%\newpage
\section{Inversa Filtrada de Matrizes}
O algoritmo (\ref{eq:dtheta}) pode ser generalizado para inverter matrizes.
%
Considere a seguinte planta MIMO não-linear de $1^{a}$ ordem  descrita por
%
\begin{equation}
\label{eq:dymimo}
{\dot {\bf y}} = {\bf K}(t)\,{\bf u} \,,
\end{equation}
%
onde ${\bf u}\!\in\!\mathbb{R}^{n}$ é a entrada da planta ou a  variável  de
controle do sistema, ${\bf y}\!\in\!\mathbb{R}^{m}$ é a saída da planta, 
${\bf K}\!\in\!\mathbb{R}^{m\times n}$ é uma função matricial não-linear e $m \leq n$. Assumindo que o objetivo de controle seja rastrear uma trajetória
%   de
%referência 
desejada ${\bf y}_d(t)$, uma lei de controle ${\bf u}$ que lineariza
o sistema e garante a estabilidade assintótica do erro de rastreamento ${\bf e}\!:=\!{\bf y}_d-{\bf y}$ é dada por
%
\begin{equation}
\label{eq:um}
{\bf u} = {\bf K}^{\dagger}(t)\,{\boldsymbol \nu} \,, \qquad {\boldsymbol \nu}={\dot{\bf y}}_d + {\boldsymbol \Lambda}\,{\bf e} \,,
\end{equation}
%
onde ${\bf K}^{\dagger}(t)\!\in\!\mathbb{R}^{n\times m}$ é a pseudo-inversa à direita da matriz ${\bf K}(t)$ e ${\boldsymbol \Lambda}\!=\!{\boldsymbol \Lambda}^T > 0$. Novamente, considera-se a substituição da pseudo-inversa %, computada instantaneamente como ${\bf K}^{\dagger}(t)$, 
por uma matriz ${\boldsymbol \Theta}(t)$ atualizada dinamicamente. Assim, para estabelecer uma dinâmica adequada para ${\boldsymbol \Theta}(t)$, introduz-se os seguintes sinais de erro não-lineares para a inversa à direita
%
\begin{equation}
\label{eq:er}
{\bf S}_{r} = {\bf K} {\boldsymbol \Theta} - {\bf I} \in \mathbb{R}^{m\times m},
\end{equation}
%
e para a inversa à esquerda
%
\begin{equation}
\label{eq:el}
{\bf S}_{\ell} = {\boldsymbol \Theta} {\bf K} - {\bf I} \in \mathbb{R}^{n\times n}.
\end{equation}
%

\newpage
\subsection{Matriz de Erro à Direita}

Considere inicialmente o erro (\ref{eq:er}) e a função de
Lyapunov
%
%\begin{equation}
$2\,V_{r}\!=\!\text{tr}({\bf S}_{r}^{T}\,{\bf S}_{r})$, % = \sum_{i=1}^m{\sum_{j=1}^m{s^2_{r_{ij}}}},$%\end{equation}
%
onde $\text{tr}(\cdot)$ corresponde à função traço. 
%
A partir de (\ref{eq:er}), a derivada temporal de $V_{r}$ ao longo  das
trajetórias do sistema é dada por
%
%\begin{eqnarray}
\begin{equation}
\label{eq:dVr}
2\,{\dot V}_{r} = \text{tr}({\dot {\bf S}}_{r}^{T}\,{\bf S}_{r} + {\bf S}_{r}^{T}\,{\dot {\bf S}}_{r}) %\nonumber \\
                = 2\text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta}) + 2\text{tr}({{\bf S}}_{r}^{T}\,{\bf K}\,{\dot {\boldsymbol \Theta}}) \,.
\end{equation}
%\end{eqnarray}
%
Em vista de (\ref{eq:dVr}), escolhe-se a seguinte lei de atualização
%
\begin{equation}
\label{eq:dThetaR}
{\dot {\boldsymbol \Theta}} = -{\boldsymbol \Gamma} {\bf K}^{T}\,{\bf S}_{r},
\end{equation}
%
onde ${\boldsymbol \Gamma} = {\boldsymbol \Gamma}^{T} > 0 $ é uma matriz de ganho de atualização. Como resultado, tem-se
%
\begin{equation}
\label{eq:dVr1}
{\dot V}_{r} = \text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta})\, \underbrace{- \,\text{tr}({{\bf S}}_{r}^{T}\,{\bf K}\,{\boldsymbol \Gamma}\,{\bf K}^{T}\,{\bf S}_{r})}_{\leq 0}\,,
\end{equation}
%
que depende de $\dot{\bf K}$. Para $\dot{\bf K} \equiv {\bf 0}$, tem-se que $\dot{V}_r \leq 0$. Neste caso, $\dot{V}_r = 0$ quando ${\bf K}^T\,{\bf S}_r = {\bf 0}$, isto é, quando as colunas de ${\bf S}_r$ pertencerem ao espaço nulo à esquerda de ${\bf K}$. Note que a igualdade ${\bf K}^T\,{\bf S}_r = {\bf 0}$ corresponde à equação normal para as colunas de ${\boldsymbol \Theta}$, dada por ${\bf K}^T({\bf K}{\boldsymbol \Theta}) = {\bf K}^T({\bf I})$.
%e ${\bf K}$ apresenta posto completo por colunas, tem-se que $V_r < 0$. Para o caso em que ${\bf K}$ apresenta posto imcompleto, $V_r = 0$ também quando as colunas de $S_r$ pertencerem ao espaco nulo à esquerda de ${\bf K}$. 
%\newpage

\subsection{Matriz de Erro à Esquerda}

Similarmente, para o erro (\ref{eq:el}) considera-se a função 
de Lyapunov %, definida positiva
%
%
$2\,V_{\ell}\!=\!\text{tr}({\bf S}_{\ell}^{T}\,{\bf S}_{\ell})$,
%
com derivada temporal ao longo das trajetórias do sistema dada por
%
%\begin{eqnarray}
\begin{equation}
\label{eq:dVl}
2\,{\dot V}_{\ell} = \text{tr}({\dot {\bf S}}_{\ell}^{T}\,{\bf S}_{\ell} + {\bf S}_{\ell}^{T}\,{\dot {\bf S}}_{\ell})% \nonumber \\
                   = 2\text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}}) + 2\text{tr}({{\bf S}}_{\ell}^{T}\,{\dot {\boldsymbol \Theta}}\,{\bf K}) \,.
\end{equation}
%\end{eqnarray}
%
Novamente, uma escolha para a lei de adaptação é:
%
\begin{equation}
\label{eq:dThetaL}
{\dot {\boldsymbol \Theta}} = -{\boldsymbol \Gamma} {\bf S}_{\ell}\,{\bf K}^{T} \,,
\end{equation}
%
onde ${\boldsymbol \Gamma} = {\boldsymbol \Gamma}^T > 0$ é a matriz de ganho de atualização, resultando em
%
\begin{equation}
\label{eq:dVl1}
{\dot V}_{\ell} = \text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}})\,\underbrace{-\, \text{tr}({\bf K}\,{{\bf S}}_{\ell}^{T}\,{\boldsymbol \Gamma}\,{\bf S}_{\ell}\,{\bf K}^{T})}_{\leq 0}\,.
\end{equation}
%
que também depende de $\dot{\bf K}$. Para $\dot{\bf K} \equiv {\bf 0}$, tem-se que $\dot{V}_{\ell} \leq 0$. Neste caso, $\dot{V}_{\ell} = 0$ quando $ {\bf S}_\ell\,{\bf K}^T = {\bf 0}$, ou seja, quando as linhas de ${\bf S}_\ell$ pertencerem ao espaço nulo de ${\bf K}$. Note também que a igualdade ${\bf S}_\ell\,{\bf K}^T = {\bf 0}$ corresponde à equação normal para as linhas de ${\boldsymbol \Theta}$, dada por ${\bf K}({\bf K}^T{\boldsymbol \Theta}^T) = {\bf K}({\bf I})$.

\subsection{Lei de Atualização Composta}
%Para matrizes $A$ retangulares com posto completo por linhas, {\it{rank($A$)}} $= m$, configura-se um caso de existência, em que o número de soluções para $A x = b$ é $1$ ou $\infty$. Neste caso, a melhor inversa à direita é $C = A^T(A A^T)^{-1}$ e $x = C b$ não apresentará componentes no espaço nulo de $A$. Para matrizes $A$ retangulares com posto completo por colunas, {\it{rank($A$)}} $= n$, configura-se um caso de unicidade. Se houver solução para $A x = b$, ela deve ser $x = BA x = B b$. Pode não haver solução, isto é, o número de soluções é $0$ ou $1$. A melhor inversa à esquerda é $B = (A^TA)^{-1}A^T$ e $p = A(B b)$ corresponde à projeção de $b$ no espaço coluna de $A$. Note que em matrizes retangulares não se pode ter ambas as condições de existência e unicidade. 

Para casos não-redundantes ($m\!=\!n$) e com posto completo, as leis de atualização (\ref{eq:dThetaR}) e (\ref{eq:dThetaL}) são capazes de resolver individualmente o problema de determinação da inversa, uma vez que as matrizes inversa à esquerda e inversa à direita são iguais. Para os casos em que $m\!\neq n$, configuram-se os casos de existência (posto completo por linhas) ou unicidade (posto completo por colunas). Nota-se que as próprias matrizes de erro ${\bf S}_r$ e ${\bf S}_{\ell}$ apresentam dimensões diferentes, sugerindo também propriedades diferentes. A lei de atualização composta que envolve as duas matrizes de erro simultaneamente pode ser obtida a partir da análise de estabilidade da função de Lyapunov $2{V}_c = 2{V}_{r} + 2{V}_{\ell}$, com derivada temporal dada por

\begin{equation}
\label{eq:dVc}
2\,{\dot V}_c = 2g(\dot{{\bf K}}) + 2\text{tr}({{\bf S}}_{r}^{T}\,{\bf K}\,{\dot {\boldsymbol \Theta}}) + 2\text{tr}({{\bf S}}_{\ell}^{T}\,{\dot {\boldsymbol \Theta}}\,{\bf K}) \,,
\end{equation}

\noindent onde $g(\dot{\bf K}) = {\text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}}) + \text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta})}$. Assim, de (\ref{eq:dVc}) tem-se
%
%\begin{equation}
%g(\dot{\bf K}) = {\text{tr}({{\bf S}}_{\ell}^{T}\,{\boldsymbol \Theta}\,{\dot {\bf K}}) + \text{tr}({{\bf S}}_{r}^{T}\,{\dot {\bf K}}\,{\boldsymbol \Theta})}.
%\end{equation}
%
%Assim, de (\ref{eq:dVc}) tem-se
%
\begin{equation}
{\dot V}_c = g(\dot{\bf K}) + \text{tr}(({{\bf S}}_{r}^{T}\,{\bf K}\, + {\bf K}\,{{\bf S}}_{\ell}^{T}\,){\dot {\boldsymbol \Theta}}).
\end{equation}
%
Desta forma, a lei de atualização proposta é 
%
\begin{equation}
\label{eq:adapcomp}
{\dot {\boldsymbol \Theta}} = -{\boldsymbol \Gamma}( {\bf S}_{\ell}\,{\bf K}^{T} + {\bf K}^{T}\,{\bf S}_{r}) \,.
\end{equation} 
%
onde ${\boldsymbol \Gamma} = {\boldsymbol \Gamma}^T > 0$. Como resultado, tem-se
%
\begin{equation}
{\dot V}_c = g(\dot{\bf K})\,\underbrace{-\,\text{tr}(({{\bf S}}_{r}^{T}\,{\bf K}\, + {\bf K}\,{{\bf S}}_{\ell}^{T}\,){\boldsymbol \Gamma}( {\bf S}_{\ell}\,{\bf K}^{T} + {\bf K}^{T}\,{\bf S}_{r}))}_{\leq 0}.
\end{equation}
%
Para $\dot{\bf K} \equiv {\bf 0}$, tem-se que $\dot{V}_c \leq 0$. Neste caso, $\dot{V}_c = 0$ quando 
%
\begin{equation}
{{\bf K}^T{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}}\,.
\end{equation} 
%
De imediato, tem-se que
%
%\begin{equation}
%{\bf K}^T\,{\bf S}_r = {\bf 0}\;\wedge\;{\bf S}_\ell\,{\bf K}^T = {\bf 0} \Rightarrow {{\bf K}^T\,{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}}\,,
%\end{equation}
%
a ocorrência simultanea das condições ${\bf K}^T\,{\bf S}_r = {\bf 0}$ e ${\bf S}_\ell\,{\bf K}^T = {\bf 0}$ apresentadas anteriormente implica em $\dot{V}_c = 0$. A prova no sentido contrário é obtida para um ganho escalar $\Gamma > 0$, através da expansão de (\ref{eq:dVc}) como%, que pode ser reescrita, para $\dot{\bf K} \equiv {\bf 0}$, como
%
\begin{equation}
{\dot V}_c =  -\Gamma\,\text{tr}({{\bf S}}_{r}^{T}\,{\bf K}\,{\bf K}^T{\bf S}_{r})
-\Gamma\,\text{tr}({\bf K}\,{{\bf S}}_{\ell}^{T}\,{\bf S}_{\ell}\,{\bf K}^T)
-2\,\Gamma\,\text{tr}({\bf W}^T{\bf W}),  
\end{equation}
%
onde ${\bf W} = {\bf S}_r\,{\bf K} = {\bf K}\,{\bf S}_\ell = {\bf K}\,{\boldsymbol \Theta}\,{\bf K} - {\bf K}$. Como todos os termos da soma são negativos ou nulos, tem-se que $\dot{V}_c = 0$ implica em 
${\bf K}^T\,{\bf S}_r = {\bf 0}$ e ${\bf S}_\ell\,{\bf K}^T = {\bf 0}$. % e, uma vez que, ${{\bf K}^T\,{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}} \Rightarrow \dot{V}_c = 0$, tem-se
%
%\begin{equation}
%{{\bf K}^T\,{\bf S}_r + {\bf S}_\ell\,{\bf K}^T = {\bf 0}} \Rightarrow {\bf K}^T\,{\bf S}_r = {\bf 0}\;\wedge\;{\bf S}_\ell\,{\bf K}^T = {\bf 0}.
%\end{equation}
%
Na próxima seção, esta nova proposta é analisada, sendo justificada a utilização das duas matrizes de erro ${\bf S}_{\ell}$ e ${\bf S}_r$. Além disso, será estabelecida uma relação entre os casos escalar e multivariável.% A única exigência para a convergência do método é que a trajetória desejada não tenda (mais lentamente do que uma exponencial) para uma singularidade. 
%Como será visto na seção \ref{sec::convanalysis}, a utilização de cada uma das matrizes de erro possui . Desta forma, é proposta uma lei de atualização que considere as duas matrizes de erro, , simultaneamente.

%$\begin{equation}
%\label{eq:adapcomp}
%{\dot \Theta} = -\Gamma( S_{\ell} K^{T} + K^{T} S_{r}) \,.
%\end{equation} 
%
%Na próxima seção, esta nova proposta é analisada, sendo justificada a utilização das duas matrizes de erro $S_{\ell}$ e $S_r$. Além disso, é estabelecida uma relação entre os casos escalar e multivariável.% A única exigência para a convergência do método é que a trajetória desejada não tenda (mais lentamente do que uma exponencial) para uma singularidade. 

\newpage
\section{\label{sec::convanalysis}Análise de Convergência}

Considere a {\it decomposição de valor singular} 
%ou {\it single value decomposition} 
(SVD) da matriz de ganho ${\bf K}(t)\!\in\!\mathbb{R}^{m\times n}$ 
%
\begin{equation}
{\bf K} = {\bf U}\,{\boldsymbol \Sigma}\,{\bf V}^T\,,
\end{equation}
%
onde ${\bf U}\!\in\!\mathbb{R}^{m\times m}$, ${\boldsymbol \Sigma}\!\in\!\mathbb{R}^{m\times n}$ e ${\bf V}\!\in\!\mathbb{R}^{n\times n}$. As matrizes ${\bf U}$ e ${\bf V}$ são matrizes ortogonais %cujas colunas são os autovetores de ${\bf K}\,{\bf K}^T$ e ${\bf K}^T\,{\bf K}$, respectivamente, 
que fornecem bases ortonormais para todos os subespaços fundamentais de ${\bf K}$, e a matriz ${\boldsymbol \Sigma}$ possui $r$ valores singulares em sua diagonal. %, as raízes dos autovalores não nulos de ${\bf K}\,{\bf K}^T$ e ${\bf K}^T\,{\bf K}$. 
A fatoração SVD escolhe as bases de forma especial 
e permite%: quando {\bf K} multiplica uma coluna ${\bf v}_j$ de ${\bf V}$, produz $\sigma_j$ vezes uma coluna de ${\bf U}$.% Isto se origina diretamente de 
%
%\begin{equation}
%{\bf K}\,{\bf V} = {\bf U}\,{\boldsymbol \Sigma}\,,
%\end{equation}
%
%observado uma coluna por vez \cite{STRANG}. 
, para um sistema (\ref{eq:dymimo}), relacionar a entrada ${\bf u}$ e a saída $\dot{\bf y}$. Seja ${\bf u}$ uma combinação linear das colunas de ${\bf V}$. A saída $\dot{\bf y}$ será a mesma combinação linear das colunas de ${\bf U}$, multiplicadas pelos valores singulares $\sigma_i$ a elas associados. 
%Assim, a fatoração SVD será utilizada para análise, se mostrando uma ferramenta muito poderosa. 
Isto se origina diretamente de 
%
%\begin{equation}
${\bf K}\,{\bf V} = {\bf U}\,{\boldsymbol \Sigma}$, observado uma coluna por vez \cite{STRANG}. 
%
Da mesma forma, pode-se expressar a matriz ${\boldsymbol \Theta}$ como
%
\begin{equation}
  {\boldsymbol \Theta} = {\bf U}_{\Theta}\,{\boldsymbol \Sigma}_{\Theta}\,{\bf V}_{\Theta}^T\,.
\end{equation}
%
A condição ${\bf K} {\boldsymbol \Theta} \rightarrow {\bf I}_m$ pode então ser dividida em:
%
\begin{eqnarray}
\label{c1} {\bf U}_\Theta  &\rightarrow& {\bf V}\,, \\
\label{c2} {\boldsymbol \Sigma}\,{\boldsymbol \Sigma}_\Theta  &\rightarrow& {\bf I}_m\,, \\ 
\label{c3} {\bf V}_\Theta  &\rightarrow& {\bf U}\,. 
\end{eqnarray}
%
Note que (\ref{c2}) representa um ajuste de amplitudes, similar ao caso escalar, e que para a condição ${\boldsymbol \Theta}\,{\bf K} \rightarrow {\bf I}_n$ se torna 
%
%\begin{equation}
%\label{c4} 
${\boldsymbol \Sigma}_\Theta\,{\boldsymbol \Sigma}  \rightarrow {\bf I}_n$. 
%\end{equation}
%
Porém, adicionalmente a este problema, estão as condições (\ref{c1}) e (\ref{c3}), associadas ao alinhamento das bases ortonormais definidas por ${\bf V}$ e ${\bf U}_\Theta$, e também ${\bf U}$ e ${\bf V}_\Theta$. Isto é, para o caso multivariável, além do ajuste de amplitudes existente no caso escalar, há também a necessidade do ajuste de ângulos. A solução adotada para análise é expressar o mapeamento realizado por ${\boldsymbol \Theta}$ através das bases ortonormais definidas por ${\bf U}$ e ${\bf V}$. Assim, assuma que
%
\begin{equation}
\label{eq::expretheta}
{\boldsymbol \Theta} = {\bf V}\,{\boldsymbol \Psi}\,{\bf U}^T\,.
\end{equation}
%
Intuitivamente, o que se faz é utilizar o mapeamento contrário, o que é razoável uma vez que ${\boldsymbol \Theta}$ deve evoluir para a inversa de ${\bf K}$. Note porém que a matriz ${\boldsymbol \Psi}$ não é diagonal e, portanto, quando ${\boldsymbol \Theta}$ multiplica uma coluna ${\bf u}_j$ de ${\bf U}$, produz uma combinação linear de todas as colunas de ${\bf V}$. 
%A Figura \ref{fig:espacos} ilustra os subespaços fundamentais e o mapeamento realizado pelas matrizes $K$ e $\Theta$.
%
%A similaridade entre os casos escalar e multivariável pode ser verificada através da análise dos elementos diagonais e residuais de $\Psi = V^T \Theta U$. 
Considere o caso mais simples em que $\dot{\bf K}\equiv {\bf 0}$ ($\dot{\bf U} \equiv {\bf 0}$ e $\dot{\bf V} \equiv {\bf 0}$). 
Neste caso, a dinâmica de ${\boldsymbol \Psi}$ é dada por
%
\begin{equation}
  \dot{\boldsymbol \Psi} = {\bf V}^T\,\dot{\boldsymbol \Theta}\,{\bf U}\,.
\end{equation}	
%
Aplicando a lei de atualização composta, proposta em (\ref{eq:adapcomp}), e considerando um ganho escalar $\Gamma > 0$ tem-se
%Using the composite update law proposed in (\ref{eq:adapcomp}) and considering a scalar gain $\Gamma > 0$ we obtain
%
\begin{align}
\label{eq::phidyn}
  \dot{\boldsymbol \Psi} &= -\Gamma {\bf V}^T({\bf S}_{\ell}\,{\bf K}^{T} + {\bf K}^{T}\,{\bf S}_{r}){\bf U} \nonumber \\
	&= -\Gamma\,{\bf V}^T\,{\bf S}_{\ell}\,{\bf K}^{T}\,{\bf U} - \Gamma\,{\bf V}^T\,{\bf K}^{T}\,{\bf S}_{r}\,{\bf U} \notag \\
	&= -\Gamma\,{\bf V}^T\,{\bf S}_{\ell}\,{\bf V}\,{\boldsymbol \Sigma}^{T}\,{\bf U}^T\,{\bf U} - \Gamma\,{\bf V}^T\,{\bf V}\,{\boldsymbol \Sigma}^{T}\,{\bf U}^T\,{\bf S}_{r}\,{\bf U} \notag \\
	&= -\Gamma\,{\bf V}^T\,({\boldsymbol \Theta}\,{\bf K} - {\bf I}_n){\bf V}{\boldsymbol \Sigma}^T - \Gamma\,{\boldsymbol \Sigma}^T\,{\bf U}^T\,({\bf K} {\boldsymbol \Theta} - {\bf I}_m){\bf U} \notag \\
   &= -\Gamma({\bf V}^T\,{\boldsymbol \Theta}\,{\bf U}{\boldsymbol \Sigma} - {\bf I}_n){\boldsymbol \Sigma}^T - \Gamma\,{\boldsymbol \Sigma}^T ({\boldsymbol \Sigma}\,{\bf V}^T\,{\boldsymbol \Theta}\,{\bf U} - {\bf I}_m) \notag \\%
	&= -\Gamma({\boldsymbol \Psi}\,{\boldsymbol \Sigma} - {\bf I}_n ){\boldsymbol \Sigma}^T - \Gamma\,{\boldsymbol \Sigma}^T({\boldsymbol \Sigma}\,{\boldsymbol \Psi} - {\bf I}_m)\,.
\end{align}					
%
A similaridade entre os casos escalar e multivariável pode ser verificada através da análise dos elementos diagonais e residuais de ${\boldsymbol \Psi}$. Desta forma, a matriz ${\boldsymbol \Psi}$ é decomposta em
%The matrix $\Psi$ can be decomposed as
%
%%% analysis of the dynamics of $\Psi$ is divided in the analysis of its diagonal and off diagonal elements. Thus, it is considered
%
\begin{equation}
\label{eq::decompRD1}
  {\boldsymbol \Psi} = {\bf D}_{\Theta} + {\bf R}_{\Theta}\,,
\end{equation}
%
onde ${\bf D}_{\Theta} \in {\mathbb R}^{n\times m}$ possui os elementos diagonais de ${\boldsymbol \Psi}$ e ${\bf R}_{\Theta} \in {\mathbb R}^{n\times m}$ possui os elementos fora da diagonal de ${\boldsymbol \Psi}$, denominados residuais, com zeros na diagonal. Por exemplo, ${\boldsymbol \Psi} \in {\mathbb R}^{4\times 3}$ pode ser expressa como
%
\begin{equation}
{\boldsymbol \Psi} = \underbrace{\left[\begin{matrix} 
							d_{1}&0&0\\
							0&d_{2}&0\\
							0&0&d_{3} \\
							0&0&0 \\
							\end{matrix} \right]}_{{\bf D}_{\Theta}}
						+
							\underbrace{\left[\begin{matrix} 
							0&r_{12}&r_{13}\\
							r_{21}&0&r_{23}\\
							r_{31}&r_{32}&0 \\
							r_{41}&r_{42}&r_{43} \\
							\end{matrix} \right]}_{{\bf R}_{\Theta}}.
\end{equation}
%
A derivada temporal de (\ref{eq::decompRD1}) é dada por 
%
\begin{equation}
\label{eq::decompRD2}
\dot{\boldsymbol \Psi} = \dot{\bf D}_{\Theta}+\dot{\bf R}_{\Theta}
\end{equation} 
%
e, substituindo (\ref{eq::decompRD1}) e (\ref{eq::decompRD2}) em (\ref{eq::phidyn}), 
%
\begin{align}
\label{eq:dynDeR}
  \dot{\bf D}_{\Theta} + \dot{\bf R}_{\Theta} = &- \Gamma\,({{\bf D}_{\Theta}}\,{\boldsymbol \Sigma} + {{\bf R}_{\Theta}}\,{\boldsymbol \Sigma}
    - {\bf I}_n){\boldsymbol \Sigma}^T \notag \\%
	&- \Gamma\,{\boldsymbol \Sigma}^T({\boldsymbol \Sigma}\,{{\bf D}_{\Theta}} + {\boldsymbol \Sigma}\,{{\bf R}_{\Theta}} - {\bf I}_m)\,.
\end{align}
%
Note que o produto de matrizes diagonais e matrizes residuais também é residual, apresentando zeros na diagonal principal. Desta forma, é possível separar a análise das dinâmicas de ${\bf D}_{\Theta}$ e ${\bf R}_{\Theta}$, presentes em (\ref{eq:dynDeR}). Assim, para a componente diagonal de ${\boldsymbol \Psi}$, tem-se
%
\begin{align}
  \dot{\bf D}_{\Theta} = -\Gamma\,({\bf D}_{\Theta}\,{\boldsymbol \Sigma} - {\bf I}_n){\boldsymbol \Sigma}^T - \Gamma\,{\boldsymbol \Sigma}^T({\boldsymbol \Sigma}\,{\bf D}_{\Theta} - {\bf I}_m )\,.
\end{align}
%
A dinâmica do elemento diagonal $d_i$ é dada por
%
\begin{align}
  \dot{d}_i = - \Gamma (d_i \sigma_i - 1)\sigma_i - \Gamma \sigma_i(\sigma_i d_i - 1)% \notag \\
	= - 2\Gamma S_i\sigma_i\,,
\end{align}
%
apresentando uma forma similar ao caso escalar analisado inicialmente (note que $\sigma_i \in \mathbb{R}^{+}$). Note também que
%
%\begin{align}
%  \dot{d}_i &= - 2\Gamma\sigma_i^2\,d_i + 2\Gamma \sigma_i\,,
%\end{align}
%
%isto é, 
os elementos da diagonal convergem exponencialmente para $1/\sigma_i$ quando $\sigma_i \neq 0$. Para $\sigma_i = 0$, tem-se que $\dot{d}_i = 0$. A Figura \ref{fig:locusS3} apresenta a evolução de $d_i$ para diferentes condições iniciais.

\begin{figure}[htpb]
\centering
\def\Ga{\framebox{\parbox[c]{12mm}{$S_i>0$ \\ $\dot{d_i}<0$}}}
\def\Gb{\framebox{\parbox[c]{12mm}{$S_i<0$ \\ $\dot{d_i}>0$}}}
\def\JPicScale{0.65}
{\small
\input{fig/plane3.pst}
}
\caption{Plano $d_i \times \sigma_i$ (diferentes condições iniciais).}
\label{fig:locusS3}
\end{figure}

Para os elementos residuais tem-se 
%
\begin{equation}
\label{eq::residue}
  \dot{{\bf R}}_{\Theta} = -\underbrace{{\Gamma {\bf R}_{\Theta}\,{\boldsymbol \Sigma}\,{\boldsymbol \Sigma}^T }}_{1}
    - \underbrace{{\Gamma\,{\boldsymbol \Sigma}^T\,{\boldsymbol \Sigma}\,{\bf R}_{\Theta}}}_{2}\,.
\end{equation}
%
As matrizes ${\boldsymbol \Sigma}\,{\boldsymbol \Sigma}^T\!\in\!{\mathbb R}^{m \times m}$ e ${\boldsymbol \Sigma}^T\,{\boldsymbol \Sigma}\!\in\!{\mathbb R}^{n \times n}$ são matrizes diagonais cujos elementos são os quadrados dos valores singulares de ${\bf K}$. Para os casos em que $m \neq n$, apresentam dimensões diferentes e uma delas também apresentará zeros na diagonal. %Por exemplo, para $\Sigma \in {\mathbb R}^{2\times 3}$:
%
%\begin{equation}
%{\Sigma\Sigma^T} = \left[\begin{matrix} %
%							\sigma_{1}^2&0\\
%							0&\sigma_{2}^2\\
%							\end{matrix} \right]
%							\quad\quad \text{e} \quad\quad
%{\Sigma^T\Sigma} = \left[\begin{matrix} 
%							\sigma_{1}^2&0&0\\
%							0&\sigma_{2}^2&0\\
%							0&0&0\\
%							\end{matrix} \right].			
%\end{equation}
%
No termo 1 de (\ref{eq::residue}), obtido quando considerada a matriz de erro ${\bf S}_\ell$, os elementos diagonais multiplicam as colunas de ${\bf R}_\Theta$. Por outro lado, no termo 2, obtido com a utilização de ${\bf S}_r$, os elementos diagonais são aplicados às linhas de ${\bf R}_\Theta$. Assim, a dinâmica de um elemento residual $r_{ij}$ pode ser expressa por
%
\begin{equation}
  \dot{r}_{ij} + \Gamma(\sigma_i^{*2}+\sigma_j^{*2})r_{ij} = 0\,,
\end{equation}
%
onde $\sigma_k^* = \sigma_k$ se $k \leq \min(m,n)$ e $\sigma_k^* = 0$ caso contrário. A utilização de ambas as matrizes de erro ${\bf S}_r$ e ${\bf S}_\ell$ torna a convergência do elemento residual dependente de dois valores singulares distintos, associados à linha e à coluna. 

\noindent {\bf Observação 1:} Considere o caso onde $m=n$ e $\sigma_m = 0$. O uso de uma lei de atualização baseada apenas em ${\bf S}_r$ (multiplicação das linhas) resultaria em $\dot{r}_{mj} = 0,\;\forall j$. Os elementos residuais de toda $m$-ésima linha não podem se alterar e portanto não se anulam. O algoritmo composto que também utiliza ${\bf S}_\ell$ (multiplicação das colunas) assegura a convergência desses elementos para $0$ desde que $\sigma_j \neq 0$. Note que uma lei de atualização baseada apenas na matriz de erro ${\bf S}_\ell$ resultaria em $\dot{r}_{in} = 0,\;\forall i$. Neste caso, todos os elementos residuais da $n$-ésima coluna não podem se alterar e, consequentemente, não convergem para $0$.
\vskip 0.5cm

\noindent {\bf Observação 2:} Para os casos onde $m\neq n$ a necessidade de uma lei de atualização composta se torna ainda mais evidente. Considere o caso ${\bf K} \in {\mathbb R}^{2\times 3}$% e com posto completo por linhas
%
\begin{equation}
{\bf K} = \begin{bmatrix} %
							\vdots&\vdots\\
							{\bf u}_1&{\bf u}_2\\%
							\vdots&\vdots\\
							\end{bmatrix} 
\begin{bmatrix} 
							\sigma_1&0&0\\
							0&\sigma_2&0\\
							\end{bmatrix} 
\begin{bmatrix} 
							\vdots&\vdots&\vdots\\
							{\bf v_1}&{\bf v_2}&{\bf v_3}\\%
							\vdots&\vdots&\vdots\\
							\end{bmatrix}^T,
\end{equation}
%
onde $\sigma_1 \neq 0$ e $\sigma_2 \neq 0$ (posto completo por linhas). A última coluna de ${\bf V}$, ${\bf v}_3$, fornece uma base ortonormal para o espaço nulo de ${\bf K}$, isto é, seja ${\bf b} = \alpha {\bf v}_3$, tem-se ${\bf K}\,{\bf b} = {\bf 0}$, $\forall\,\alpha$. A matriz ${\boldsymbol \Theta}$ pode ser expressa, de acordo com (\ref{eq::expretheta}), como
%
\begin{equation}
{\boldsymbol \Theta} = \begin{bmatrix} %
							\vdots&\vdots&\vdots\\
							{\bf v}_1&{\bf v}_2&{\bf v}_3\\%
							\vdots&\vdots&\vdots\\
							\end{bmatrix} 
\begin{bmatrix} 
							d_1&r_{12}\\
							r_{21}&d_2\\
							r_{31}&r_{32}\\
							\end{bmatrix}
\begin{bmatrix} 
							\vdots&\vdots\\
							{\bf u}_1&{\bf u}_2\\%
							\vdots&\vdots\\
							\end{bmatrix}^T.
\end{equation}
%
Note que $r_{31}$ e $r_{32}$ estão associados a esta base (multiplicam ${\bf v}_3$) e, para $r_{31} \neq 0$ e $r_{32} \neq 0$, uma lei ${\bf b} = {\boldsymbol \Theta} {\bf c}$ apresentará componentes no espaço nulo de ${\bf K}$, não representando portanto uma lei de norma mínima ou ótima. Como foi visto, apenas a utilização de ${\bf S}_\ell$ permitirá que estes elementos evoluam para zero, uma vez que torna a convergência dependente do valor singular associado à coluna. 
\vskip 0.5cm

\noindent {\bf Observação 3:} Considere agora o caso ${\bf K} \in {\mathbb R}^{3\times 2}$, com posto completo por colunas ($\sigma_1 \neq 0$ e $\sigma_2 \neq 0$). Da mesma forma, ${\bf K}$ pode ser escrito como
%
\begin{equation}
{\bf K} = 		\begin{bmatrix} %
							\vdots&\vdots&\vdots\\
							{\bf u}_1&{\bf u}_2&{\bf u}_3\\%
							\vdots&\vdots&\vdots\\
							\end{bmatrix} 
							\begin{bmatrix}
							\sigma_1&0\\
							0&\sigma_2\\
							0&0\\
							\end{bmatrix} 
							\begin{bmatrix} 
							\vdots&\vdots\\
							{\bf v}_1&{\bf v}_2\\%
							\vdots&\vdots\\
							\end{bmatrix}^T\,.
\end{equation}
%
Neste caso, a última coluna de ${\bf U}$, ${\bf u}_3$, fornece uma base ortonormal para o espaço nulo à esquerda de ${\bf K}$. Isto é, para um sistema ${\bf K} {\bf b} = {\bf c}$, não existe solução ${\bf b}$ para ${\bf c} = \alpha {\bf u}_3$. Representa portanto uma base para variáveis ${\bf c}$ inalcançáveis ou inviáveis. Considere novamente a expressão proposta em (\ref{eq::expretheta}) para ${\boldsymbol \Theta}$
%
\begin{equation}
{\boldsymbol \Theta} = \begin{bmatrix} %
							\vdots&\vdots\\
							{\bf v}_1&{\bf v}_2\\%
							\vdots&\vdots\\
							\end{bmatrix} 
\begin{bmatrix} 
							d_1&r_{12}&r_{13}\\
							r_{21}&d_2&r_{23}\\
							\end{bmatrix} 
\begin{bmatrix} 
							\vdots&\vdots&\vdots\\
							{\bf u}_1&{\bf u}_2&{\bf u}_3\\%
							\vdots&\vdots&\vdots\\
							\end{bmatrix} ^T.
\end{equation}
%
Os elementos residuais $r_{13}$ e $r_{23}$ estão associados a ${\bf u}_3$. Para $r_{13}\neq 0$, $r_{23}\neq 0$ e ${\bf c} = \alpha {\bf u}_3$, tem-se ${\bf b} = {\boldsymbol \Theta} {\bf c} = r_{13}{\bf v}_1 + r_{23}{\bf v}_2 \neq {\bf 0}$, embora ${\bf c}$ seja uma saída inviável para o sistema. Neste caso, apenas a utilização da matriz de erro à direita ${\bf S}_r$ garantirá que estes elementos residuais convirjam para $0$ e assim ${\bf b} = {\boldsymbol \Theta} {\bf c} \rightarrow 0$. 


\section{Solução para Cinemática Inversa}

Como foi visto no capítulo \ref{chap::cinematica}, a solução para o problema de cinemática inversa pode ser obtida de forma analítica, envolvendo o uso de identidades algébricas e geométricas, ou através de métodos numéricos e técnicas de otimização. A solução diferencial, também denominada algoritmo iterativo, foi apresentada nas seções \ref{sec::nonred} e \ref{sec::red}, e tem como base o mapeamento linear realizado pela matriz Jacobiana entre as velocidades nos espaços das juntas e operacional. Como visto, a escolha%Como foi visto,
%
%
%omo foi visto no capítulo \ref{chap::cine}, a solução para o problema de cinemática inversa pode ser obtida através da forma fechada, ou analítica, envolvendo o uso de identidades algébricas e geométricas para solucionar as equações não lineares que definem a cinemática do manipulador, ou através de métodos numéricos e técnicas de otimização. Como visto, as equacoes de cinematica diferencial representam um mapeamento linear entre as velocidades das juntas e no espaco operacional sugerindo a possibilidade de usar tal relacao para solucionar a cinematica inversa. A solucao atraves de uma abordagem diferencial, muitas vezes denominada algoritmo iterativo, foi apresentada nas seções \ref{sec::nred} e \ref{sec::red}, sendo aqui reescrita por conveniencia.
%
\begin{equation}
\dot{\bf q} = {\bf J}^{-1}\,{\boldsymbol \nu},
\end{equation}
% 
para ${\boldsymbol \Lambda} = {\boldsymbol \Lambda}^T > 0$ e ${\boldsymbol \nu}=\dot{\bf x}_d + {\boldsymbol \Lambda}{\bf e}$ garante a convergência exponencial do erro ${\bf e}$ para ${\bf 0}$. Para manipuladores redundantes, a pseudo-inversa à direita ${\bf J}^{\dagger} = {\bf J}^T({\bf J}{\bf J}^T)^{-1}$ é utilizada, fornecendo uma solução de norma mínima ou ótima desde que ${\boldsymbol \nu}(t)$ não conduza o manipulador para configurações singulares. Nestas configurações, a matriz ${\bf J}$ perde posto e sua inversa não se define. Na vizinhança de singularidades, a matriz Jacobiana se torna mal condicionada, apresentando pivôs muito pequenos e resultando em velocidades de juntas elevadas, indesejadas na prática. A proposta de aplicação da inversa filtrada como solução para a cinemática inversa de manipuladores robóticos é dada por 
%
\begin{equation}
\label{eq::cinvini}
\dot{\bf q} = {\boldsymbol \Theta}\,{\boldsymbol \nu}
\end{equation}
%
onde ${\boldsymbol \Theta}\!\in\!{\mathbb R}^{n \times m}$ é a inversa filtrada da matriz Jacobiana ${\bf J}$, sendo atualizada pela lei composta apresentada em (\ref{eq:adapcomp}), logo,
%
\begin{equation}
\dot{\boldsymbol \Theta} = -{\boldsymbol \Gamma}({\bf J}^T{\bf S}_r + {\bf S}_\ell{\bf J}^T)
\end{equation}
%
onde ${\boldsymbol \Gamma} = \gamma {\bf I} > 0$ é o ganho de atualização. As matrizes ${\bf S}_r$ e ${\bf S}_\ell$ correspondem às matrizes de erro à direita e à esquerda, respectivamente.
%
%
%
%
%%%%%%%%%%%%%%%%%%% SIMULACOES 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TRAJ1
\begin{simulation}
\label{sim::zebra_traj1}
\end{simulation}

\noindent Considere novamente o manipulador antropomórfico Zebra-ZERO, porém sem atuação nas últimas 3 juntas. Desta forma, configura-se um caso 3DoF, não redundante para posição ${\bf p}\!\in\!\mathbb{R}^3$. Inicialmente será avaliado o desempenho do algoritmo proposto para a \nameref{trajze1}, livre e afastada de singularidades (Figura \ref{fig::zebra_traj1}). 

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 0cm 0cm 0cm 0cm, scale = 0.39]{sim/zebra_traj1.eps}&
\includegraphics[trim = 0cm 0cm 2cm 0cm, scale = 0.39]{sim/zebra_traj1_2.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj1}: manipulador Zebra-ZERO e trajetória desejada em perspectiva e visão superior.}
\label{fig::zebra_traj1}
\end{figure}

Os resultados obtidos para ${\bf q}(0) = [0\;{\pi}/2\;-\pi]$, ${\boldsymbol \Theta}(0) = {\bf 0}$, $\Lambda = 2\,{\bf I}$ e ${\boldsymbol \Gamma} = {\bf I}$ são apresentados a seguir e comparados com os obtidos pelo algoritmo DLS ($\delta_0 = 3\cdot10^2$ e $\omega_0 = 10^3$). Como esperado, os dois métodos possibilitaram o rastreamento com erro praticamente nulo. As trajetórias efetuadas e as normas dos erros de posição são mostradas nas Figuras \ref{fig::cartesiano_1} e \ref{fig::zebra1_data}.a, respectivamente. 

\begin{figure}[!htb]
\includegraphics[trim = 0cm 0cm 0cm 0cm, scale = 0.39]{sim/zebra/traj1/cartesiano.eps}
\caption{Simulação \ref{sim::zebra_traj1}: trajetórias no espaço cartesiano.}
\label{fig::cartesiano_1}
\end{figure}

\newpage
\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj1/norm.eps}\\
\includegraphics[trim = 2.7cm 0cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj1/cond.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj1}: (a) norma do erro e (b) manipulabilidade.}
\label{fig::zebra1_data}
\end{figure}

A medida de manipulabilidade $\omega({\bf q}) = \sqrt{\det{\bf J}({\bf q}){\bf J}^{T}({\bf q})}$ é mostrada na Figura \ref{fig::zebra1_data}.b e evidencia o fato de que a trajetória desejada esta afastada de configurações singulares. Desta forma, $\delta = 0$ para todo tempo $t$. A Figura \ref{fig::zebra1_dataq} apresenta as variáveis $\dot{\bf q}$ e ${\bf q}$ obtidas. 

\begin{figure}[!htb]
\includegraphics[trim = 2.35cm 0cm 2.6cm 0cm, scale = 0.49]{sim/zebra/traj1/dataq.eps}
\caption{Simulação \ref{sim::zebra_traj1}: variáveis (a) $\dot{\bf q}_{IF}$; (b) $\dot{\bf q}_{DLS}$; (c) ${\bf q}_{IF}$; e (d) ${\bf q}_{DLS}$}.
\label{fig::zebra1_dataq}
\end{figure}

Note que os resultados são praticamente coincidentes e que as velocidades das juntas permanecem numa faixa adequada, abaixo de 0.2 rad/s após o transitório.  





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TRAJ2
\newpage
\begin{simulation}
\label{sim::zebra_traj2}
\end{simulation}

\noindent Considera-se agora uma trajetória que apresenta singularidades internas, isto é, configurações singulares que pertencem ao espaço de trabalho do manipulador robótico. A trajetória considerada (\nameref{trajze2}) apresenta 3 pontos de singularidade de ombro, como evidenciado na Figura \ref{fig::zebra_traj2}. 

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 0cm 0cm 0cm 2cm, scale = 0.39]{sim/zebra_traj2.eps}&
\includegraphics[trim = 0cm 0cm 2cm 0cm, scale = 0.39]{sim/zebra_traj2_2.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj2}: manipulador Zebra-ZERO e trajetória desejada em perspectiva e visão frontal.}
\label{fig::zebra_traj2}
\end{figure}

As trajetórias desenvolvidas são mostradas nas Figuras \ref{fig::cartesiano_2} e \ref{fig::zebra2_plane}. Nota-se que, para os ganhos considerados, o algoritmo proposto apresentou um desempenho superior ao método DLS, permitindo o rastreamento da trajetória desejada com erro praticamente nulo, inclusive em configurações singulares. As normas dos erros de posição e as medidas de manipulabilidade são apresentados na Figura \ref{fig::zebra2_data}.

\begin{figure}[!htb]
\centering
\includegraphics[trim = 1.8cm 0cm 0cm 0cm, scale = 0.39]{sim/zebra/traj2_1/cartesiano.eps}
\caption{Simulação \ref{sim::zebra_traj2}: trajetórias no espaço cartesiano.}
\label{fig::cartesiano_2}
\end{figure}

\newpage
\begin{figure}[!htb]
\centering
\includegraphics[trim = 2.35cm 0.5cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj2_1/plane.eps}\\
\caption{Simulação \ref{sim::zebra_traj2}: trajetórias no plano $yz$.}
\label{fig::zebra2_plane}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj2_1/norm.eps}\\
\includegraphics[trim = 2.7cm 0cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj2_1/norm_amp.eps}\\
\includegraphics[trim = 2.7cm 0.5cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj2_1/cond.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj2}: (a) norma do erro; (b) norma do erro (escala ampliada); e (c) manipulabilidade.}
\label{fig::zebra2_data}
\end{figure}

\newpage

As variáveis $\dot{\bf q}$ e ${\bf q}$ obtidas são apresentadas na Figura \ref{fig::zebra2_dataq}. Note que as velocidades das juntas obtidas com a inversa filtrada são mais suaves, uma vez que não há o chaveamento de um termo de amortecimento. 
 
\begin{figure}[!htb]
\includegraphics[trim = 2.35cm 0cm 2.6cm 0.3cm, scale = 0.49]{sim/zebra/traj2_1/dataq_amp_only.eps}
\caption{Simulação \ref{sim::zebra_traj2}: variáveis (a) $\dot{\bf q}_{IF}$; (b) $\dot{\bf q}_{DLS}$; (c) ${\bf q}_{IF}$; e (d) ${\bf q}_{DLS}$}
\label{fig::zebra2_dataq}
\end{figure}

Em configurações de pouca manipulabilidade, onde o termo de amortecimento é adicionado, a convergência da inversa filtrada também se torna mais lenta, visto que estas configurações estão associadas a um ou mais valores singulares pequenos. A Figura \ref{fig::zebra2_delta} apresenta a variável de amortecimento $\delta$ utilizada pelo algoritmo DLS e o valor da função $V_c$, havendo uma similaridade entre ambos os sinais.

\begin{figure}[!htb]
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0cm 2.6cm 0cm, scale = 0.49]{sim/zebra/traj2_1/vc.eps}\\
\includegraphics[trim = 2.7cm 0.3cm 2.6cm 0cm, scale = 0.49]{sim/zebra/traj2_1/delta.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj2}: (a) função $V_c$ e (b) $\delta$.}
\label{fig::zebra2_delta}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TRAJ 2 - condicao 2
\newpage
Para uma condição inicial ${\bf q}(0) = [0\;(\pi/2-0.1)\;-(\pi/2-0.1)]$, dentro da região de pouca manipulabilidade definida por $\omega_0$, o algoritmo DLS apresenta desempenho pouco satisfatório, sendo incapaz de rastrear a posicao desejada (Figuras \ref{fig::cartesiano_22} e \ref{fig::zebra22_data}). Neste caso, a variável $\delta > 0$ para todo tempo $t$ (Figura \ref{fig::zebra22_data}.c), garantindo uma solução viável porém pouca precisa.
 
\begin{figure}[!htb]
\centering
\includegraphics[trim = 0cm 0.5cm 0cm 0.5cm, scale = 0.39]{sim/zebra/traj2_2/cartesiano.eps}
\caption{Simulação \ref{sim::zebra_traj2}b: trajetórias no espaço cartesiano.}
\label{fig::cartesiano_22}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0.2cm 2.7cm 1cm, scale = 0.49]{sim/zebra/traj2_2/norm.eps}\\
\includegraphics[trim = 2.7cm 0.1cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/traj2_2/cond.eps}\\
\includegraphics[trim = 2.7cm 0.2cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/traj2_2/delta.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj2}b: (a) norma do erro; (b) manipulabilidade; e (c) $\delta$.}
\label{fig::zebra22_data}
\end{figure}

%\begin{figure}[!htb]
%\includegraphics[trim = 2.35cm 0.2cm 2.6cm 0.8cm, scale = 0.49]{sim/zebra/traj2_2/delta.eps}
%\caption{Legend}
%\label{fig::zebra22_delta}
%\end{figure}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TRAJ 3
\newpage
\begin{simulation}
\label{sim::zebra_traj3}
\end{simulation}
\noindent Neste caso, analisa-se o desempenho dos métodos para um caso extremo, em que trechos da trajetória desejada não pertencem ao espaço de trabalho do manipulador, de forma que o erro não pode ser anulado. A trajetória considerada (\nameref{trajze3}) é similar à considerada na simulação \ref{sim::zebra_traj1}, porém deslocada no eixo $x$.

%\noindent In this case, we analyze the performance of the methods for an extreme case, in which part of the desired trajectory
%is outside the workspace of the manipulator, so that the error cannot be cancelled. A trajetória considera (Trajetória \ref{trajze3}) é similar to trajectory I but shifted in the positive xaxis. 

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 0cm 0cm 0cm 0cm, scale = 0.39]{sim/zebra_traj3.eps}&
\includegraphics[trim = 0cm 0cm 2cm 0cm, scale = 0.39]{sim/zebra_traj3_2.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj3}: manipulador Zebra-ZERO e trajetória desejada em perspectiva e visão superior.}
\label{fig::zebra_traj3}
\end{figure}

As Figuras \ref{fig::cartesiano_3} e \ref{fig::zebra3_plane} apresentam as trajetórias desenvolvidas no espaço cartesiano. As normas dos erros e as medidas de manipulabilidade são apresentadas na Figura \ref{fig::zebra3_data}. Ambos os algoritmos permitem o rastreamento da parcela viável da trajetória desejada. Porém, em determinados instantes, esta se torna inalcançável e ambos os algoritmos levam o manipulador à configurações próximas de singularidades. 

%The norms of the errors and manipulability measures
%are presented in Figure 8. Both algorithms allow the tracking
%of the viable part of the trajectory. But at some point, the
%desired trajectory becomes unreachable and both methods
%lead the manipulator to configurations near singularities.

\begin{figure}[!htb]
\centering
\includegraphics[trim = 1.8cm 0cm 0cm 0cm, scale = 0.39]{sim/zebra/traj3_1/cartesiano.eps}
\caption{Simulação \ref{sim::zebra_traj3}: trajetórias no espaço cartesiano.}
\label{fig::cartesiano_3}
\end{figure}

\newpage
\begin{figure}[!htb]
\centering
\includegraphics[trim = 2.35cm 0.5cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj3_1/plane.eps}\\
\caption{Simulação \ref{sim::zebra_traj3}: trajetórias no plano $xy$.}
\label{fig::zebra3_plane}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj3_1/norm.eps}\\
\includegraphics[trim = 2.7cm 0cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj3_1/norm_amp.eps}\\
\includegraphics[trim = 2.7cm 0.5cm 2.7cm 0cm, scale = 0.49]{sim/zebra/traj3_1/cond.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj3}: (a) norma do erro; (b) norma do erro (escala ampliada); e (c) manipulabilidade.}
\label{fig::zebra3_data}
\end{figure}

\newpage

As variáveis $\dot{\bf q}$ e ${\bf q}$ obtidas são apresentadas na Figura \ref{fig::zebra3_dataq}. Nos trechos em que a trajetória se torna não factível, a utilização da inversa filtrada resultou em oscilações indesejadas nas velocidades das juntas e, consequentemente, na posição do efetuador. O algoritmo DLS, por sua vez, resultou em picos de velocidade na vizinhança da região de chaveamento de $\delta$. 

\begin{figure}[!htb]
\includegraphics[trim = 2.35cm 0.5cm 2.6cm 0.9cm, scale = 0.49]{sim/zebra/traj3_1/dataq.eps}
\caption{Simulação \ref{sim::zebra_traj3}: variáveis (a) $\dot{\bf q}_{IF}$; (b) $\dot{\bf q}_{DLS}$; (c) ${\bf q}_{IF}$; e (d) ${\bf q}_{DLS}$}
\label{fig::zebra3_dataq}
\end{figure}

Nota-se também que, uma vez que a trajetória é novamente alcançável, o método DLS apresenta um desvio considerável. Este desvio pode ser atribuído ao fato de que o chaveamento é realizado com base em medidas de manipulabilidade, isto é, elipsóides no espaço operacional. Os sinais $V_c$ e $\delta$ são mostrados na Figura \ref{fig::zebra3_delta}.
 
\begin{figure}[!htb]
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0cm 2.6cm 0.2cm, scale = 0.49]{sim/zebra/traj3_1/vc.eps}\\
\includegraphics[trim = 2.7cm 0.3cm 2.6cm 0cm, scale = 0.49]{sim/zebra/traj3_1/delta.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj3}: (a) função $V_c$ e (b) $\delta$.}
\label{fig::zebra3_delta}
\end{figure}

\newpage

Na Figura \ref{fig::zebra32_plane} são apresentadas as trajetórias obtidas com o método da inversa filtrada para um trecho não factível e diferentes ganhos ${\boldsymbol \Gamma} = \gamma\,{\bf I}$.  

\begin{figure}[!htb]
\centering
\includegraphics[trim = 2.35cm 0.2cm 2.7cm 0.2cm, scale = 0.49]{sim/zebra/traj3_2/cartesiano.eps}\\
\caption{Simulação \ref{sim::zebra_traj3}: trecho não factível e diferentes ganhos $\gamma$.}
\label{fig::zebra32_plane}
\end{figure}

O ganho $\gamma$ utilizado está diretamente relacionado ao desempenho do algoritmo proposto e quanto maior este ganho, menor será a amplitude das oscilações observadas no espaço operacional. Porém, ao analisar as variáveis $\dot{\bf q}$ desenvolvidas a nível de juntas, se verifica também um aumento considerável na frequência das oscilações, geralmente inviáveis e indesejadas em aplicações práticas. A Figura \ref{fig::zebra32_dataq} apresenta as velocidades e as variáveis de juntas obtidas para $\gamma = 25$. 
 
\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0.1cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/traj3_2/qdot.eps}\\
\includegraphics[trim = 2.7cm 0.1cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/traj3_2/q.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_traj3}: variáveis (a) $\dot{\bf q}_{IF}$ e (b) ${\bf q}_{IF}$ para $\gamma = 25$.} 
\label{fig::zebra32_dataq}
\end{figure}























\newpage
\section{Lei de Controle Modificada}

Considere inicialmente o ganho escalar $k$ e sua função inversa $k^{-1}$. A inversa é uma função ímpar de $k$ e possui o mesmo sinal do ganho $k$. Porém, esta igualdade de sinal não é verificada para a inversa filtrada $\theta$. %A Figura \ref{fig:dyntheta} abaixo ilustra uma curva típica no plano $\theta \times k$ para um ganho $k$ evoluindo do semi-plano positivo para o negativo.
%
%Consider the scalar gain $k$ and its inverse function $k^{-1}$. The inverse is an odd function of $k$, that is, it has the same sign of $k$. The same is not verified for the function $k$ and its filtered inverse $\theta$. Figure \ref{fig:cartesiano1} below illustrates a typical curve in $\theta \times k$ plane for a gain $k$ evolving from the positive to the negative half-plan.
%
%\begin{figure}[!htb]
%  \centering
%  \includegraphics[width=70mm]{siso_smp.eps}
%  \caption{Plane $\theta \times k$ (curva típica).}
% \label{fig:evolk1}
%\end{figure}
%
%
%\begin{figure}[htpb]
%\centering
%\def\JPicScale{0.5}%0.65
%{\small
%\begin{tabular}{cc}
%\input{fig/evol1.pst}& \qquad \qquad
%\input{fig/evol2.pst}
%\end{tabular}
%}
%\caption{Plano $\theta \times k$ (curva típica).}
%\label{fig:dyntheta}
%\end{figure}
%
%%% Assuming $\dot{k} < \infty$, it can be stated that $ \partial \theta/\partial k = 0$ for $S = 0$ or $k = 0$, defining different curves of "exit" ($k$ approaching $0$) and "arrival" ($S$ approaching $0$). Note that $\theta$ isn't an odd function of $k$, and, soon after $k$ reaches $0$, the two variables have opposite signs.
%\newpage
Com o objetivo de recuperar esta propriedade, é proposta uma modificação na forma como $\theta$ é aplicada na lei de controle $u$ em (\ref{eq:u}). A nova proposta é dada por
%
\begin{equation}
  \theta_M = \theta^2k\,.
\end{equation}
%
Desta forma, tem-se que $sign(\theta_M) = sign(k)$. A Figura \ref{fig:dyntheta} ilustra os comportamentos de $\theta$ e $\theta_M$ para um ganho $k$ evoluindo do semi-plano positivo para o negativo.

\begin{figure}[htpb]
\centering
\def\JPicScale{0.57}%0.65
{\small
\begin{tabular}{cc}
\input{fig/evol1.pst}& \qquad \qquad 
\input{fig/evol2.pst}
\end{tabular}
}
\caption{Planos $\theta \times k$ e $\theta_M \times k$ (curvas típicas).}
\label{fig:dyntheta}
\end{figure}
%
%\begin{figure}[htpb]
%\centering
%\def\JPicScale{0.65}
%{\small
%\input{fig/evol2.pst}
%}
%\caption{Plano $\theta^2k \times k$ (curva típica).}
%\label{fig:dyntheta2}
%\end{figure}

A utilização da proposta modificada resulta no mapeamento de todo o eixo $\theta$ na origem do plano $\theta_M \times k$. Além disso, uma vez que os $2^o$ e $4^o$ quadrantes são mapeados nos $3^o$ e $1^o$ quadrantes, a hipérbole passa a representar também o lugar geométrico de $S = -2$. Note porém que para $S = -2$ e $k \neq 0$, $\dot{\theta} \neq 0$. Para o caso multivariável uma modificação similar é proposta. A matriz ${\boldsymbol \Theta}_M$ é dada por
%
\begin{equation}
  {\boldsymbol \Theta}_M = {\boldsymbol \Theta}\,{\boldsymbol \Theta}^T\,{\bf K}^T\,.
\end{equation}
%
Note que ${{\boldsymbol \Theta}\,{\boldsymbol \Theta}^T}$ é semi-definida positiva e simétrica. 
Considere o problema de controle dado pela equação $\dot{\bf e} = \dot{\bf y}_d - {\bf K}\,{\bf u}$ e a função de Lyapunov $2V_e = {\bf e}^T{\boldsymbol \Lambda}^T{\bf e}$. Sua derivada temporal ao longo das trajetórias do sistema é dada por $\dot{V}_e = {\bf e}^T{\boldsymbol \Lambda}^T(\dot{\bf y}_d - {\bf K}\,{\bf u})$. Assim, para ${\bf u} = {\bf u}_{M} = {\boldsymbol \Theta}\,{\boldsymbol \Theta}^T\,{\bf K}^T\,{\boldsymbol \nu}\,$, tem-se
%
\begin{equation}\label{eq:dVe}
  \dot{V}_e = {\bf e}^T{\boldsymbol \Lambda}^T\,({\bf I}_m - {\bf N}\,{\bf N}^T)\,\dot{\bf y}_d\, \underbrace{-\,{\bf e}^T{\boldsymbol \Lambda}^T\,{\bf N}\,{\bf N}^T{\boldsymbol \Lambda}\,{\bf e}}_{\Phi}\,,
\end{equation}
%
onde ${\bf N} = {\bf K}\,{\boldsymbol \Theta}$. O termo $\Phi$ corresponde a uma parcela semi-definida negativa, independente da qualidade da estimativa dada pelo algoritmo da inversa filtrada \eqref{eq:adapcomp}. 
%No caso onde ${\bf K}$ possui posto reduzido, o uso de ${\bf u}_{_{M}}({\boldsymbol \nu}) = {\boldsymbol \Theta}_M\,{\boldsymbol \nu}$ cancela as componentes de ${\boldsymbol \nu}$ no espaço nulo à esquerda de ${\bf K}$. Isto é, para ${\boldsymbol \nu} = {\boldsymbol \nu}_c + {\boldsymbol \nu}_n $, onde ${\boldsymbol \nu}_c \in Col({\bf K})$ e ${\boldsymbol \nu}_n \in Nul({\bf K}^T)$, tem-se ${\bf u}_{_{M}}({\boldsymbol \nu}) = {\bf u}_M({\boldsymbol \nu}_c)$. 
Assim, a proposta modificada para solução do problema de cinemática inversa é dada por
%
\begin{equation}
\label{eq::cinvmod}
\dot{\bf q}_M = {\boldsymbol \Theta}_M{\boldsymbol \nu} = {\boldsymbol \Theta}{\boldsymbol \Theta}^T{\bf J}^T{\boldsymbol \nu}, 
\end{equation}
% 
A relação entre as velocidades efetuada e demandada é então obtida:
%
\begin{equation}
\dot{\bf x} = {\bf J}{\boldsymbol \Theta}{\boldsymbol \Theta}^T{\bf J}^T{\boldsymbol \nu} = {\bf M}\,{\boldsymbol \nu}.
\end{equation}
%
Note que ${\bf M} = {\bf M}^T \geq 0$, apresentando forma similar à matriz de projeção no espaço coluna de ${\bf J}$, dada por ${\bf P} = {\bf J}({\bf J}^T {\bf J})^{-1}{\bf J}^T$. %Desta forma, velocidades demandadas nas direções dos autovetores de ${\bf M}$ resultam em velocidades efetuadas na mesma direção e sentido. 
% e, portanto, pode ser fatorada (SVD) como ${\bf M} = {\bf Q} {\boldsymbol \Sigma}_M {\bf Q}^T$. %Desta forma, velocidades demandadas nas direções das colunas de ${\bf Q}$ (autovetores de ${\bf M}$) resultam em velocidades efetuadas na mesma direção e sentido, com ajuste de amplitude definido pelos valores positivos da diagonal de ${\boldsymbol \Sigma}_M$ (. 
%Assim, oscilações como as observadas na simulação \ref{sim::zebra_traj3} envolvendo a proposta inicial (\ref{eq::cinvini}) não serão observadas. 
Para ${\boldsymbol \nu} = {\boldsymbol \nu}_c + {\boldsymbol \nu}_n $, onde ${\boldsymbol \nu}_c \in Col({\bf J})$ e ${\boldsymbol \nu}_n \in Nul({\bf J}^T)$, tem-se $\dot{\bf q}_{M}({\boldsymbol \nu}) = \dot{\bf q}_{M}({\boldsymbol \nu}_c)$, isto é, a utilização de $\dot{\bf q}_{M}({\boldsymbol \nu}) = {\boldsymbol \Theta}_M\,{\boldsymbol \nu}$ cancela as componentes de ${\boldsymbol \nu}$ no espaço nulo à esquerda de ${\bf J}$. 
Note também que ${\dot{V}_c}$ é função da variável de erro ponderada ${\bf e}_w = {\boldsymbol \Lambda}{\bf e}$. Desta forma, espera-se que para casos em que a variável de referência é inalcançável, a matriz ${\boldsymbol \Lambda}$ influencie na solução obtida, ponderando os diferentes objetivos de controle. Uma simulação envolvendo orientação e posição inatingíveis simultaneamente será realizada, servindo como análise preliminar desta propriedade. Porém, uma análise rigorosa é necessária e deve ser incluída em trabalhos futuros.

\begin{simulation}
\label{sim::zebra_mod}
\end{simulation}

\noindent Considere a mesma trajetória e os mesmos parâmetros da simulação \ref{sim::zebra_traj3}. 
%No que se segue, é avaliado o desempenho da proposta modificada, apresentada em (\ref{eq::cinvini}), para trechos onde a posição desejada se torna inalcançável. 
As Figuras \ref{fig::zebramod_plane} e \ref{fig::zebramod_plane2} apresentam as trajetórias obtidas com o algoritmo DLS e a partir da proposta modificada para dois ganhos de atualização $\Gamma = \gamma{\bf I}$. 
%Como será visto, esta nova proposta permite que sinais de velocidade nas direções de pouca manipulabilidade sejam atenuados e oscilações de alta frequência nas variáveis de juntas não serão observadas.

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 3.2cm 1.2cm 2.7cm 0.7cm, scale = 0.49]{sim/zebra/mod/plane.eps}\\
%\includegraphics[trim = 3.2cm 0.5cm 2.7cm 0cm, scale = 0.49]{sim/zebra/mod/plane_amp.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_mod}: trajetórias no plano $xy$.}
\label{fig::zebramod_plane}
\end{figure}
%\newpage

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
%\includegraphics[trim = 3.2cm 1cm 2.7cm 0.8cm, scale = 0.49]{sim/zebra/mod/plane.eps}\\
\includegraphics[trim = 2.7cm 0.5cm 2.7cm 0cm, scale = 0.49]{sim/zebra/mod/plane_amp.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_mod}: trechos de transição no plano $xy$.}
\label{fig::zebramod_plane2}
\end{figure}

Para $\gamma = 1$, a solução obtida a partir da inversa filtrada é bastante similar à obtida pelo algoritmo DLS, apresentando um desvio considerável no momento em que a trajetória se torna novamente viável. Porém, nota-se que as variáveis $\dot{\bf q}$ obtidas com a inversa filtrada são mais suaves nos pontos de transição, notavelmente quando o manipulador se aproxima da singularidade (Figura \ref{fig::zebramod_dataq}). À medida que o manipulador se aproxima destas configurações, a convergência de ${\boldsymbol \Theta}$ torna-se mais lenta e os sinais de velocidade nas direções degeneradas são atenuados. %Como esperado, as oscilações de alta frequência nas variáveis de juntas não são observadas.

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0.3cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/mod/dq1.eps}\\
\includegraphics[trim = 2.7cm 0.3cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/mod/dq2.eps}\\
\includegraphics[trim = 2.7cm 0.3cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/mod/dq3.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_mod}: variáveis (a) $\dot{q_1}$; (b) $\dot{q_2}$; e (c) $\dot{q_3}$ ($\gamma = 1$).}
\label{fig::zebramod_dataq}
\end{figure}

O ganho de atualização $\Gamma$ está diretamente relacionado ao desempenho do algoritmo e, para ganhos elevados ($\gamma = 25$), os desvios de trajetória em pontos de transição são pequenos, como mostrado na Figura \ref{fig::zebramod_plane2}.  Nota-se também que as velocidades das juntas obtidas não apresentam alterações significativas em suas amplitudes (Figuras \ref{fig::zebramod_dataq} e \ref{fig::zebramod_dataq_25}), estando a principal alteração no instante de atuação. 

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0.2cm 2.7cm 0.05cm, scale = 0.49]{sim/zebra/mod/dq_25.eps}\\
\includegraphics[trim = 2.7cm 0.1cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/mod/dq_25_amp.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_mod}: $\dot{\bf q}$ em (a) escala normal e (b) ampliada ($5<t<10$).}
\label{fig::zebramod_dataq_25}
\end{figure}

%\newpage
% COMENTAR CASO COMENTE BASTANTE ANTES DA PRIMEIRA SIMULACAO!
Como esperado, a convergência das matrizes de erro se torna mais rápida para ganhos de atualização maiores (Figura \ref{fig::zebramod_cond_25}.b). Desta forma, configurações de menor manipulabilidade são atingidas (Figura \ref{fig::zebramod_cond_25}.a). 

\begin{figure}[!htb]
\centering
\begin{tabular}{cc}
\includegraphics[trim = 2.7cm 0.3cm 2.7cm -0.42cm, scale = 0.49]{sim/zebra/mod/cond.eps}\\
\includegraphics[trim = 2.7cm 0.3cm 2.7cm 0.1cm, scale = 0.49]{sim/zebra/mod/vc.eps}\\
\end{tabular}
\caption{Simulação \ref{sim::zebra_mod}: (a) manipulabilidade  e (b) função $V_c$ ($6<t<10$).}
\label{fig::zebramod_cond_25}
\end{figure}

