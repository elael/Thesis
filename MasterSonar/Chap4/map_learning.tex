
\section{Map Learning}

Map learning is an iterative optimization process to find the $\coord{w}$ that
minimizes $\text{NLL}_{\text{reg}}(\coord{w})$, equation \ref{eq:finitenll}. As
it is a convex function, gradient descent method would find it global
minimum. The gradient of the objective function is:

\begin{equation}
\label{eq:fullgrad}
\nabla\text{NLL}_{\text{reg}}(\coord{w}) =  \sum_{i=1}^n
-y_i\hat\varphi(x_i)(1+\exp(y_i~ \coord{w}\cdot\hat\varphi(x_i)))^{-1}
+\lambda\nabla\mathcal{S}(\coord{w})
\end{equation}

The gradient $\nabla\mathcal{S}(\coord{w})$ is calculated using
sub-differentials, as the $\ell_1$ part of a elastic-net
$\mathcal{S}(\coord{w})$, equation \ref{eq:elasticnet}, is non-differentiable at
$\coord{w}=\mathbf{0}$.

The gradient descent method generates a sequence of approximated values for
$\coord{w}$ by descenting on the gradient direction:

\begin{equation}
\label{eq:gd}
\coord{w}_{t+1}=\coord{w}_{t}-\eta\nabla\text{NLL}_{\text{reg}}(\coord{w}_t)
\end{equation}

Where $\eta\in\mathbb{R}^+$ is a step value and $\coord{w}_t$ is a sequence of
approximations. The issue with this approach is the cost of computing
$\nabla\text{NLL}_{\text{reg}}(\coord{w})$ for a whole map, the summation of
equation \ref{eq:fullgrad} ranges over all sampled points from all beams from
all measurements, with sampling as in section \ref{s:ism}, and it is computed
at every step of $\coord{w}_t$.

\subsection{Stochastic Gradient Descent - SGD}

To overcame the sampling size issue of gradient descent, stochastic gradient
descent proposes the use of a single, or small batch, of samples at
a time. One first shuffle the tranning samples~\cite{bottou2012stochastic} then
directly update $\coord{w}_t$ as:

\begin{subequations}
\begin{equation}
\label{eq:sgd}
\coord{w}_{t+1}=\coord{w}_{t}-\eta_t\nabla\text{NLL}_{\text{reg}}(\coord{w}_t;(y_t,x_t))
\end{equation}
\begin{equation}
\nabla\text{NLL}_{\text{reg}}(\coord{w}_t;(y_t,x_t)) = -y_t\hat\varphi(x_t)(1+\exp(y_t~ \coord{w}\cdot\hat\varphi(x_t)))^{-1}
+\lambda\nabla\mathcal{S}(\coord{w})
\end{equation}
\end{subequations}

Where $(y_t,x_t)$ are the shuffled version of $(y_i,x_i)$. The mini-batch
variation~\cite{li2014efficient} of this method takes partition the set of
samples $\underset{\scriptscriptstyle k}{\sqcup}I_k = \{(y_i,x_i)|i=1\ldots n\}$ and suffle,
then the update equation becomes:

\begin{subequations}
\begin{equation}
\label{eq:minibatch}
\coord{w}_{t+1}=\coord{w}_{t}-\eta_t\nabla\text{NLL}_{\text{reg}}(\coord{w}_t;I_t)
\end{equation}
\begin{equation}
\nabla\text{NLL}_{\text{reg}}(\coord{w}_t;I_t) =
\sum_{(y_i,x_i)\in I_t} -y_t\hat\varphi(x_i)(1+\exp(y_i~
\coord{w}\cdot\hat\varphi(x_i)))^{-1} +\lambda\nabla\mathcal{S}(\coord{w})
\end{equation}
\end{subequations}

The algorithm is garanteed to converge (under mild
conditions~\citet{bottou2012stochastic}), given that $\sum_t \eta_t^2<\infty$
and $\sum_t \eta_t = \infty$. A classic choice for $\eta_t$ is

\begin{equation}
\label{eq:classicstep}
\eta_t = \frac{\eta_0}{1+\nicefrac{t}{n}}
\end{equation}

Where $\eta_0$ is a initial step determined from a small sample
and $n$ is the number of samples. Variations of this form also are
commum, \citet{ramos2016hilbert} provides another choice:

\begin{equation*}
\eta_t = \frac{1}{\lambda\alpha_2(t_0+t)}
\end{equation*}

Where $\lambda$ is the regulator gain, equation \ref{eq:minibatch}, $\alpha_2$
is the $\ell_2$ elastic-net gain, equation \ref{eq:elasticnet}, and $t_0$ is
chosen form a small sample test. However, \citet{tsuruoka2009stochastic} adopts
a exponetial decay for $\eta_t$, which is not complient with theoretical
requiremnts, and they had a better result then using equation
\ref{eq:classicstep}. The reason provided was that a harmonic progression
decays too fast at the beginning and too slowly at the end. As a trade-off, this
work employs a theoretically valid step that do not suffer
from the aforementioned limitations:

\begin{equation}
\label{eq:leariningrate}
\eta_t = \frac{\eta_0}{2}\left(\frac{1}{(1+\nicefrac{t}{n})\log_n(n+t)} +
k_1\e^{-\nicefrac{t}{n}}\right)
\end{equation}

Where $\eta_0$, similarly to the classical case, is a initial step. The
ratinale is to acomodate a slower decay at the beginning,
dictated by the exponetial component, and faster at the end without losing the
divergence, as $\sum_x x\log x$ still divergent. Faster decaying end rates can
always be found, if needed.
